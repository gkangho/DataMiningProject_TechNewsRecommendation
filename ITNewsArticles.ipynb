{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IT News Project"
      ],
      "metadata": {
        "id": "AL6DdQVg5sDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project aims to analyze IT news articles and recommend related articles to identify patterns, trends. This could help in understanding certain topics or discovering emerging themes in current events."
      ],
      "metadata": {
        "id": "hTOODAm5590e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Scraping and Crawling"
      ],
      "metadata": {
        "id": "eAuG2qJ967pP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use multiple websites to scrape the articles. Each website has a different snippet of code as the method to get the articles is adapted to each website."
      ],
      "metadata": {
        "id": "CzvOCE7E7EjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Code to execute before the others, all the other scraping snippets can be executed individually though"
      ],
      "metadata": {
        "id": "rhyVuACsLgtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for the first time launching google collab (packages to install)\n",
        "!pip install python-dotenv beautifulsoup4 python-dateutil"
      ],
      "metadata": {
        "id": "xdncfhiTLno5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "745f1e6e-e7a4-46bb-9eca-f2a144585008"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to execute each time (imports used everywhere)\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import json\n",
        "import time\n",
        "import dateutil\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "nJCM0UflGIE6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### New York Times Articles\n",
        "New York Times has its own API so we will use it. More specifically we will use the article search API."
      ],
      "metadata": {
        "id": "pv5vhOJx7kUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Execute this code before to create the .env file for the NYT API Key\n",
        "\n",
        "**If you want to retrieve New York Times articles**\n"
      ],
      "metadata": {
        "id": "wNUKNg-kNCko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('.env', 'w') as f:\n",
        "    f.write(\"O5OF6OugazREsfmSUZtiPm69XGAcDZte\\n\")"
      ],
      "metadata": {
        "id": "jRc-jU3XF6b1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Code for New York Times"
      ],
      "metadata": {
        "id": "nNHiqcbWN2A1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib3\n",
        "from dotenv import load_dotenv\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "#load api key\n",
        "load_dotenv()\n",
        "\n",
        "####################\n",
        "####PARAMETERS######\n",
        "####################\n",
        "API_KEY = os.getenv('API_KEY')  # my API key retrieved from NYT API service\n",
        "NYT_base_url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json'# base URL for the NYT API\n",
        "\n",
        "# function to send a request to the NYT API\n",
        "def send_request(page):\n",
        "    params = {\n",
        "        'q': 'technology',\n",
        "        'api-key': API_KEY,\n",
        "        'page': page\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # make a GET request to the NYT API\n",
        "        response = requests.get(NYT_base_url, params=params, verify=False)\n",
        "        response.raise_for_status()  # raise an error for HTTP issues\n",
        "        time.sleep(7)  # delay to prevent rate-limiting (They allow 10 requests per minute so to make sure to respect that)\n",
        "        return response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for page {page}: {e}\")\n",
        "        return None\n",
        "\n",
        "# function to check if the article is well structured\n",
        "def is_valid(article):\n",
        "    # check if the article has a headline and main text\n",
        "    return 'headline' in article and 'main' in article['headline']\n",
        "\n",
        "# function to parse the response data into a DataFrame\n",
        "def parse_response(response):\n",
        "    # initialize data structure fields\n",
        "    data = {\n",
        "        'URL': [],\n",
        "        'Title': [],\n",
        "        'Author': [],\n",
        "        'Publication Date': [],\n",
        "        'Content': [], # content will be empty since the NYT doesn't give access to the full text\n",
        "        'Keywords': []\n",
        "    }\n",
        "\n",
        "    # extract information from each article\n",
        "    articles = response['response']['docs']\n",
        "    for article in articles:\n",
        "        if is_valid(article):\n",
        "            # append relevant data to the dictionary\n",
        "            data['URL'].append(article['web_url'])\n",
        "            data['Title'].append(article['headline']['main'])\n",
        "            data['Author'].append(article.get('byline', {}).get('original', \"No Author Found\"))\n",
        "            data['Publication Date'].append(dateutil.parser.parse(article['pub_date']).date())\n",
        "            data['Content'].append(\"\")  #empty since there is no text\n",
        "            keywords = [keyword['value'] for keyword in article['keywords'] if keyword['name'] == 'subject']\n",
        "            data['Keywords'].append(\", \".join(keywords) if keywords else \"No Keywords Found\")# store keywords as a comma-separated string\n",
        "\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# function to collect articles from multiple pages and save them as a single CSV\n",
        "def get_NYT_multiple_pages(nb_pages):\n",
        "    all_articles = pd.DataFrame()\n",
        "\n",
        "    #loop through multiple pages to fetch articles\n",
        "    for page_num in range(nb_pages):\n",
        "        print(f\"Fetching page {page_num + 1}...\")\n",
        "        response = send_request(page_num)\n",
        "        if response is not None:\n",
        "            page_data = parse_response(response)\n",
        "            all_articles = pd.concat([all_articles, page_data], ignore_index=True)\n",
        "        else:\n",
        "            print(f\"Skipping page {page_num + 1} due to errors.\")\n",
        "\n",
        "    #check and remove if any duplicate URLs\n",
        "    if 'URL' in all_articles.columns:\n",
        "        initial_count = len(all_articles)\n",
        "        all_articles = all_articles.drop_duplicates(subset='URL').reset_index(drop=True)\n",
        "        final_count = len(all_articles)\n",
        "        print(f\"Removed {initial_count - final_count} duplicate articles based on URL.\")\n",
        "    else:\n",
        "        print(\"No 'URL' column found. Skipping duplicate check.\")\n",
        "\n",
        "    #save the final DataFrame as a single CSV file\n",
        "    csv_path = \"/content/nyt_tech_articles.csv\"\n",
        "    all_articles.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"Data saved to {csv_path}\")\n",
        "\n",
        "\n",
        "# specify the number of pages to scrape\n",
        "get_NYT_multiple_pages(50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nitzXANDyZPg",
        "outputId": "d2e7cce5-14c1-4e76-d69e-6138b2b55af3",
        "collapsed": true
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "Fetching page 4...\n",
            "Fetching page 5...\n",
            "Fetching page 6...\n",
            "Error fetching data for page 5: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=5\n",
            "Skipping page 6 due to errors.\n",
            "Fetching page 7...\n",
            "Error fetching data for page 6: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=6\n",
            "Skipping page 7 due to errors.\n",
            "Fetching page 8...\n",
            "Error fetching data for page 7: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=7\n",
            "Skipping page 8 due to errors.\n",
            "Fetching page 9...\n",
            "Error fetching data for page 8: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=8\n",
            "Skipping page 9 due to errors.\n",
            "Fetching page 10...\n",
            "Error fetching data for page 9: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=9\n",
            "Skipping page 10 due to errors.\n",
            "Fetching page 11...\n",
            "Error fetching data for page 10: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=10\n",
            "Skipping page 11 due to errors.\n",
            "Fetching page 12...\n",
            "Error fetching data for page 11: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=11\n",
            "Skipping page 12 due to errors.\n",
            "Fetching page 13...\n",
            "Error fetching data for page 12: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=12\n",
            "Skipping page 13 due to errors.\n",
            "Fetching page 14...\n",
            "Error fetching data for page 13: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=13\n",
            "Skipping page 14 due to errors.\n",
            "Fetching page 15...\n",
            "Error fetching data for page 14: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=14\n",
            "Skipping page 15 due to errors.\n",
            "Fetching page 16...\n",
            "Error fetching data for page 15: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=15\n",
            "Skipping page 16 due to errors.\n",
            "Fetching page 17...\n",
            "Error fetching data for page 16: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=16\n",
            "Skipping page 17 due to errors.\n",
            "Fetching page 18...\n",
            "Error fetching data for page 17: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=17\n",
            "Skipping page 18 due to errors.\n",
            "Fetching page 19...\n",
            "Error fetching data for page 18: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=18\n",
            "Skipping page 19 due to errors.\n",
            "Fetching page 20...\n",
            "Error fetching data for page 19: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=19\n",
            "Skipping page 20 due to errors.\n",
            "Fetching page 21...\n",
            "Error fetching data for page 20: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=20\n",
            "Skipping page 21 due to errors.\n",
            "Fetching page 22...\n",
            "Error fetching data for page 21: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=21\n",
            "Skipping page 22 due to errors.\n",
            "Fetching page 23...\n",
            "Error fetching data for page 22: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=22\n",
            "Skipping page 23 due to errors.\n",
            "Fetching page 24...\n",
            "Error fetching data for page 23: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=23\n",
            "Skipping page 24 due to errors.\n",
            "Fetching page 25...\n",
            "Error fetching data for page 24: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=24\n",
            "Skipping page 25 due to errors.\n",
            "Fetching page 26...\n",
            "Error fetching data for page 25: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=25\n",
            "Skipping page 26 due to errors.\n",
            "Fetching page 27...\n",
            "Error fetching data for page 26: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=26\n",
            "Skipping page 27 due to errors.\n",
            "Fetching page 28...\n",
            "Error fetching data for page 27: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=27\n",
            "Skipping page 28 due to errors.\n",
            "Fetching page 29...\n",
            "Error fetching data for page 28: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=28\n",
            "Skipping page 29 due to errors.\n",
            "Fetching page 30...\n",
            "Error fetching data for page 29: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=29\n",
            "Skipping page 30 due to errors.\n",
            "Fetching page 31...\n",
            "Error fetching data for page 30: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=30\n",
            "Skipping page 31 due to errors.\n",
            "Fetching page 32...\n",
            "Error fetching data for page 31: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=31\n",
            "Skipping page 32 due to errors.\n",
            "Fetching page 33...\n",
            "Error fetching data for page 32: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=32\n",
            "Skipping page 33 due to errors.\n",
            "Fetching page 34...\n",
            "Error fetching data for page 33: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=33\n",
            "Skipping page 34 due to errors.\n",
            "Fetching page 35...\n",
            "Error fetching data for page 34: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=34\n",
            "Skipping page 35 due to errors.\n",
            "Fetching page 36...\n",
            "Fetching page 37...\n",
            "Fetching page 38...\n",
            "Fetching page 39...\n",
            "Fetching page 40...\n",
            "Fetching page 41...\n",
            "Error fetching data for page 40: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=40\n",
            "Skipping page 41 due to errors.\n",
            "Fetching page 42...\n",
            "Error fetching data for page 41: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=41\n",
            "Skipping page 42 due to errors.\n",
            "Fetching page 43...\n",
            "Error fetching data for page 42: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=42\n",
            "Skipping page 43 due to errors.\n",
            "Fetching page 44...\n",
            "Error fetching data for page 43: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=43\n",
            "Skipping page 44 due to errors.\n",
            "Fetching page 45...\n",
            "Error fetching data for page 44: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=44\n",
            "Skipping page 45 due to errors.\n",
            "Fetching page 46...\n",
            "Error fetching data for page 45: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=45\n",
            "Skipping page 46 due to errors.\n",
            "Fetching page 47...\n",
            "Error fetching data for page 46: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=46\n",
            "Skipping page 47 due to errors.\n",
            "Fetching page 48...\n",
            "Error fetching data for page 47: 429 Client Error: Too Many Requests for url: https://api.nytimes.com/svc/search/v2/articlesearch.json?q=technology&api-key=O5OF6OugazREsfmSUZtiPm69XGAcDZte&page=47\n",
            "Skipping page 48 due to errors.\n",
            "Fetching page 49...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-3661b071334e>\u001b[0m in \u001b[0;36m<cell line: 96>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# specify the number of pages to scrape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0mget_NYT_multiple_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-3661b071334e>\u001b[0m in \u001b[0;36mget_NYT_multiple_pages\u001b[0;34m(nb_pages)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpage_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_pages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Fetching page {page_num + 1}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mpage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-3661b071334e>\u001b[0m in \u001b[0;36msend_request\u001b[0;34m(page)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# make a GET request to the NYT API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNYT_base_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# raise an error for HTTP issues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# delay to prevent rate-limiting (They allow 10 requests per minute so to make sure to respect that)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Tech Articles\n",
        "We will scrape the articles from the Tech page of CNN. The CNN sitemap doesn't keep any tech related articles, and scraping/crawling on the /search is forbidden (as written on the robots.txt). This takes around 15s (only one page to crawl)"
      ],
      "metadata": {
        "id": "KMpKyJKVIcc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "####################\n",
        "####PARAMETERS######\n",
        "####################\n",
        "CNN_base_url = \"https://edition.cnn.com/business/tech\" #Tech section base URL\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"}#headers to pretend to be a browser\n",
        "CNN_url = \"https://edition.cnn.com\" #URL to construct the url manually just in case\n",
        "\n",
        "#function to extract article URLs from CNN's Business Tech section\n",
        "def get_cnn_tech_article_urls(page_num=1):\n",
        "    # construct the URL for the page\n",
        "    url = f\"{CNN_base_url}?page={page_num}\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "    article_links = set()  #use a set to automatically handle duplicates\n",
        "\n",
        "    #check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        #find all article links\n",
        "        for link in soup.find_all(\"a\", href=True):  #finds all <a> tags with an href attribute\n",
        "            href = link[\"href\"]\n",
        "\n",
        "            #look for articles with \"/tech/\" in the URL (tech-related articles)\n",
        "            if \"/2024/\" in href and \"/tech/\" in href:\n",
        "                full_url = href if href.startswith(\"http\") else CNN_url + href\n",
        "                article_links.add(full_url)\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to retrieve page {page_num} at {url}\")\n",
        "\n",
        "    return article_links\n",
        "\n",
        "#function to extract article text from JSON-LD script tag\n",
        "def extract_text_from_json_ld(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        json_ld_tag = soup.find(\"script\", type=\"application/ld+json\")\n",
        "        if json_ld_tag:\n",
        "            json_data = json.loads(json_ld_tag.string)\n",
        "            if isinstance(json_data, list):\n",
        "                for item in json_data:\n",
        "                    if \"articleBody\" in item:\n",
        "                        return item[\"articleBody\"]\n",
        "            elif \"articleBody\" in json_data:\n",
        "                return json_data[\"articleBody\"]\n",
        "        print(f\"No article body found in JSON-LD for {url}\")\n",
        "        return \"\"\n",
        "    else:\n",
        "        print(f\"Failed to retrieve page at {url}\")\n",
        "        return \"\"\n",
        "\n",
        "#function to scrape the full text of an article\n",
        "def scrape_CNN_article(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        title = soup.find('title')\n",
        "        title = title.text.strip() if title else \"No Title Found\"\n",
        "\n",
        "        author_meta = soup.find('meta', attrs={'name': 'author'})\n",
        "        author = author_meta['content'].strip() if author_meta else \"No Author Found\"\n",
        "\n",
        "        date = soup.find('meta', attrs={'property': 'article:published_time'})\n",
        "        date = dateutil.parser.parse(date['content']).date() if date else \"No Date Found\"\n",
        "\n",
        "        paragraphs = soup.find('div', attrs={'class': 'article__content'}).find_all('p')\n",
        "        content = \"\\n\".join(p.text.strip() for p in paragraphs if p.text)\n",
        "\n",
        "        return {'URL': url, 'Title': title, 'Author': author, 'Publication Date': date, 'Content': content}\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return {'URL': url, 'Title': None, 'Author': None, 'Publication Date': None, 'Content': None}\n",
        "\n",
        "#function to save articles to a CSV file\n",
        "def save_to_csv(articles, filename=\"cnn_tech_articles.csv\"):\n",
        "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=[\"URL\",\"Title\", \"Author\", \"Publication Date\", \"Content\"])\n",
        "        writer.writeheader()\n",
        "        for article in articles:\n",
        "            writer.writerow(article)\n",
        "\n",
        "def scrape_CNN_multiple_pages(num_pages=1):\n",
        "    article_urls = set()\n",
        "    articles = []\n",
        "    for page_num in range(1, num_pages+1):\n",
        "      article_urls.update(get_cnn_tech_article_urls(page_num))\n",
        "    article_urls = list(article_urls)[:100]\n",
        "    for url in article_urls:\n",
        "      articles.append(scrape_CNN_article(url))\n",
        "      time.sleep(1)  #pause to avoid overwhelming the server\n",
        "    save_to_csv(articles)\n",
        "    print(\"Articles saved to cnn_tech_articles.csv\")\n",
        "\n",
        "#run the scraper\n",
        "#(I only fetch the first page because CNN doesn't store more than the current articles on the main tech page)\n",
        "scrape_CNN_multiple_pages(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU9zCAiO4KZQ",
        "outputId": "c3704577-3357-48a2-d228-312edd68bf78"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Articles saved to cnn_tech_articles.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tech Crunch\n",
        "Tech Crunch is a news website dedicated to technological news. Therefore, we will crawl the latest news pages and scrape each article on the page. This snippet takes around 10 mins (for me)."
      ],
      "metadata": {
        "id": "gLpByc-tI8iA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Tech crunch also gives access to the sitemap so the crawling could have been done that way too.*"
      ],
      "metadata": {
        "id": "AAo0XfU_PODV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "####PARAMETERS######\n",
        "####################\n",
        "TC_base_url = \"https://techcrunch.com/latest/page/\" #base URL latest news\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"}#headers to pretend to be a browser\n",
        "\n",
        "def fetch_page(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    return response.text\n",
        "\n",
        "#function to parse article links from the page\n",
        "def parse_article_links(page_html):\n",
        "    soup = BeautifulSoup(page_html, 'html.parser')\n",
        "    article_links = []\n",
        "\n",
        "    #look for links with the correct class name\n",
        "    for article in soup.find_all('a', class_='loop-card__title-link'):\n",
        "        article_url = article.get('href')\n",
        "\n",
        "        #exclude links that contain '/podcast/'\n",
        "        if '/podcast/' not in article_url:\n",
        "            article_links.append(article_url)\n",
        "\n",
        "    return article_links\n",
        "\n",
        "#function to scrape the full text of an article\n",
        "def scrape_TC_article(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        title = soup.find('title')\n",
        "        title = title.text.strip() if title else \"No Title Found\"\n",
        "\n",
        "        author_meta = soup.find('meta', attrs={'name': 'author'})\n",
        "        author = author_meta['content'].strip() if author_meta else \"No Author Found\"\n",
        "\n",
        "        date = soup.find('meta', attrs={'property': 'article:published_time'})\n",
        "        date = dateutil.parser.parse(date['content']).date() if date else \"No Date Found\"\n",
        "\n",
        "        #try to find all <p> tags with the class 'wp-block-paragraph'\n",
        "        paragraphs = soup.find('div', attrs={'class': 'entry-content wp-block-post-content is-layout-constrained wp-block-post-content-is-layout-constrained'}).find_all('p')\n",
        "        content = \"\\n\".join(p.text.strip() for p in paragraphs if p.text)\n",
        "\n",
        "        return {'URL': url, 'Title': title, 'Author': author, 'Publication Date': date, 'Content': content}\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return {'URL': url, 'Title': None, 'Author': None, 'Publication Date': None, 'Content': None}\n",
        "\n",
        "#function to scrape multiple pages\n",
        "def scrape_TC_multiple_pages(start_page=1, end_page=5):\n",
        "    csv_filename = \"tc_tech_articles.csv\"\n",
        "    scraped_urls = set()  # Set to track scraped URLs and avoid duplicates\n",
        "\n",
        "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=['URL', 'Title', 'Author', 'Publication Date', 'Content'])\n",
        "        writer.writeheader()\n",
        "\n",
        "        for page_num in range(start_page, end_page + 1):\n",
        "            page_url = TC_base_url+f'{page_num}/'\n",
        "\n",
        "            #fetch the page\n",
        "            print(f\"Fetching page: {page_url}\")\n",
        "            page_html = fetch_page(page_url)\n",
        "\n",
        "            #extract article links from the page\n",
        "            article_urls = parse_article_links(page_html)\n",
        "\n",
        "            #scrape content from each article\n",
        "            for article_url in article_urls:\n",
        "                if article_url not in scraped_urls:  #check if URL is already scraped\n",
        "                    article_data = scrape_TC_article(article_url)\n",
        "                    if article_data['Content']:  #only save if content is non-empty\n",
        "                        writer.writerow(article_data)\n",
        "                        scraped_urls.add(article_url)  #add URL to the set\n",
        "                else:\n",
        "                  time.sleep(1)  # wait for 1 second between requests to avoid overloading server\n",
        "\n",
        "    print(f\"Scraped articles are saved to {csv_filename}\")\n",
        "\n",
        "# Run the scraper\n",
        "scrape_TC_multiple_pages(start_page=1, end_page=51)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHB6sCy9urg4",
        "outputId": "1551b9fc-abc6-4388-8fd0-4486ae59b46b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching page: https://techcrunch.com/latest/page/1/\n",
            "Fetching page: https://techcrunch.com/latest/page/2/\n",
            "Fetching page: https://techcrunch.com/latest/page/3/\n",
            "Fetching page: https://techcrunch.com/latest/page/4/\n",
            "Fetching page: https://techcrunch.com/latest/page/5/\n",
            "Fetching page: https://techcrunch.com/latest/page/6/\n",
            "Fetching page: https://techcrunch.com/latest/page/7/\n",
            "Fetching page: https://techcrunch.com/latest/page/8/\n",
            "Fetching page: https://techcrunch.com/latest/page/9/\n",
            "Fetching page: https://techcrunch.com/latest/page/10/\n",
            "Fetching page: https://techcrunch.com/latest/page/11/\n",
            "Fetching page: https://techcrunch.com/latest/page/12/\n",
            "Fetching page: https://techcrunch.com/latest/page/13/\n",
            "Fetching page: https://techcrunch.com/latest/page/14/\n",
            "Fetching page: https://techcrunch.com/latest/page/15/\n",
            "Fetching page: https://techcrunch.com/latest/page/16/\n",
            "Fetching page: https://techcrunch.com/latest/page/17/\n",
            "Fetching page: https://techcrunch.com/latest/page/18/\n",
            "Fetching page: https://techcrunch.com/latest/page/19/\n",
            "Fetching page: https://techcrunch.com/latest/page/20/\n",
            "Fetching page: https://techcrunch.com/latest/page/21/\n",
            "Fetching page: https://techcrunch.com/latest/page/22/\n",
            "Fetching page: https://techcrunch.com/latest/page/23/\n",
            "Fetching page: https://techcrunch.com/latest/page/24/\n",
            "Fetching page: https://techcrunch.com/latest/page/25/\n",
            "Fetching page: https://techcrunch.com/latest/page/26/\n",
            "Fetching page: https://techcrunch.com/latest/page/27/\n",
            "Fetching page: https://techcrunch.com/latest/page/28/\n",
            "Fetching page: https://techcrunch.com/latest/page/29/\n",
            "Fetching page: https://techcrunch.com/latest/page/30/\n",
            "Fetching page: https://techcrunch.com/latest/page/31/\n",
            "Fetching page: https://techcrunch.com/latest/page/32/\n",
            "Fetching page: https://techcrunch.com/latest/page/33/\n",
            "Fetching page: https://techcrunch.com/latest/page/34/\n",
            "Fetching page: https://techcrunch.com/latest/page/35/\n",
            "Fetching page: https://techcrunch.com/latest/page/36/\n",
            "Fetching page: https://techcrunch.com/latest/page/37/\n",
            "Fetching page: https://techcrunch.com/latest/page/38/\n",
            "Fetching page: https://techcrunch.com/latest/page/39/\n",
            "Fetching page: https://techcrunch.com/latest/page/40/\n",
            "Fetching page: https://techcrunch.com/latest/page/41/\n",
            "Fetching page: https://techcrunch.com/latest/page/42/\n",
            "Fetching page: https://techcrunch.com/latest/page/43/\n",
            "Fetching page: https://techcrunch.com/latest/page/44/\n",
            "Fetching page: https://techcrunch.com/latest/page/45/\n",
            "Fetching page: https://techcrunch.com/latest/page/46/\n",
            "Fetching page: https://techcrunch.com/latest/page/47/\n",
            "Fetching page: https://techcrunch.com/latest/page/48/\n",
            "Fetching page: https://techcrunch.com/latest/page/49/\n",
            "Fetching page: https://techcrunch.com/latest/page/50/\n",
            "Fetching page: https://techcrunch.com/latest/page/51/\n",
            "Scraped articles are saved to tc_tech_articles.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Venture Beats\n",
        "The sitemap for venturebeats archives the sites news everyday. So, we will crawl the sitemap will all the links to the archives, and then crawl each archive, and scrape all articles with the designed keyword (I choose all keywords for all the IT related categories on the main website). There are headers for each snippet of code, but this was added for Venture Beats in the first place, they are very strict with their Scraping/Crawling rules. This takes around 40 mins for 100 pages."
      ],
      "metadata": {
        "id": "RPyV7Tn5JYvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "####PARAMETERS######\n",
        "####################\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"}#headers to pretend to be a browser\n",
        "VB_sitemap_url = \"https://venturebeat.com/sitemap.xml\"\n",
        "\n",
        "#fetch main sitemap where all archives of the sitemap of everyday are stored\n",
        "def fetch_sitemap(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, \"xml\")\n",
        "    sitemap_links = [loc.text for loc in soup.find_all(\"loc\")]\n",
        "    return sitemap_links\n",
        "\n",
        "#filter tech related articles (so filter with keywords and take out additional content such as pictures, etc)\n",
        "def is_article_url(url):\n",
        "    exclude_keywords = [\"wp-content\", \".png\", \".jpg\", \".webp\"]\n",
        "    if any(keyword in url for keyword in exclude_keywords):\n",
        "        return False\n",
        "    include_keywords = [\"/ai/\", \"/data-infrastructure/\", \"/programming-development/\", \"/automation/\", \"/security/\"]\n",
        "    return any(keyword in url for keyword in include_keywords)\n",
        "\n",
        "#scrape each article\n",
        "def scrape_VB_article(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        title = soup.find('h1', class_='article-title') or soup.find('title')\n",
        "        title = title.text.strip() if title else \"No Title Found\"\n",
        "\n",
        "        author_meta = soup.find('meta', attrs={'name': 'author'})\n",
        "        author = author_meta['content'].strip() if author_meta else \"No Author Found\"\n",
        "\n",
        "        date = soup.find('time')\n",
        "        date = dateutil.parser.parse(date['datetime']).date() if date else \"No Date Found\"\n",
        "\n",
        "        article_content = soup.find(\"div\", class_=\"article-content\")\n",
        "        #remove unwanted elements with the specified class names\n",
        "        for unwanted in article_content.find_all([\"div\", \"form\"], class_=[\"post-boilerplate\", \"boilerplate-before\", \"boilerplate-after\"]):\n",
        "            unwanted.decompose()\n",
        "\n",
        "        if article_content:\n",
        "            #extract only the text from the relevant section, ensuring it is cleaned up\n",
        "            clean_text = article_content.get_text(separator=\"\\n\", strip=True)\n",
        "        else:\n",
        "            print(\"Article content not found!\")\n",
        "            clean_text = \"\"\n",
        "\n",
        "        return {'URL': url, 'Title': title, 'Author': author, 'Publication Date': date, 'Content': clean_text}\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return {'URL': url, 'Title': None, 'Author': None, 'Publication Date': None, 'Content': None}\n",
        "\n",
        "#fetch a certain number of days of archive on the main sitemap\n",
        "def fetch_daily_sitemaps(main_sitemap, num_days=100):\n",
        "    daily_sitemaps = []\n",
        "    for i in range(min(num_days, len(main_sitemap))):\n",
        "        daily_sitemaps.append(main_sitemap[i])\n",
        "    return daily_sitemaps\n",
        "\n",
        "def scrape_VB_multiple_pages(num_days=100):\n",
        "    main_sitemap = fetch_sitemap(VB_sitemap_url)\n",
        "    daily_sitemaps = fetch_daily_sitemaps(main_sitemap, num_days)\n",
        "    #collect articles url\n",
        "    all_urls = set()\n",
        "    for daily_sitemap in daily_sitemaps:\n",
        "        daily_urls = fetch_sitemap(daily_sitemap)\n",
        "        all_urls.update(daily_urls)\n",
        "        time.sleep(5)  #sleep to not overwhelm the server\n",
        "\n",
        "    #filter URLs with the filtering method\n",
        "    article_urls = [url for url in all_urls if is_article_url(url)]\n",
        "\n",
        "    #scrape the articles one by one\n",
        "    articles_data = []\n",
        "    for url in article_urls:\n",
        "        article_data = scrape_VB_article(url)\n",
        "        if article_data['Content']:  # Save only when they have content\n",
        "            articles_data.append(article_data)\n",
        "        time.sleep(5)\n",
        "\n",
        "    output_file = \"/content/vb_tech_articles.csv\"\n",
        "    df = pd.DataFrame(articles_data)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Scraping complete. Data saved to {output_file}.\")\n",
        "\n",
        "#run the scraper\n",
        "scrape_VB_multiple_pages(100)"
      ],
      "metadata": {
        "id": "xXxwBJu_4Mrc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b249d60e-0595-4a64-a562-2cb12d71ebf4",
        "collapsed": true
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping complete. Data saved to /content/vb_tech_articles.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each dataframe has been created according to the source code of each website, and has been checked for duplicates. The delay is also adapted to the robots.txt of the website if indicated."
      ],
      "metadata": {
        "id": "pbcCaWy6Ohyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Data Preprocessing"
      ],
      "metadata": {
        "id": "VFatrMm3Kkgr"
      }
    }
  ]
}