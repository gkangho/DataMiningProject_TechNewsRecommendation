{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IT News Project"
      ],
      "metadata": {
        "id": "AL6DdQVg5sDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project aims to analyze IT news articles and recommend related articles to identify patterns, trends. This could help in understanding certain topics or discovering emerging themes in current events."
      ],
      "metadata": {
        "id": "hTOODAm5590e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Scraping and Crawling"
      ],
      "metadata": {
        "id": "eAuG2qJ967pP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use multiple websites to scrape the articles. Each website has a different snippet of code as the method to get the articles is adapted to each website."
      ],
      "metadata": {
        "id": "CzvOCE7E7EjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Code to execute before the others, all the other scraping snippets can be executed individually though"
      ],
      "metadata": {
        "id": "rhyVuACsLgtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for the first time launching google collab (packages to install)\n",
        "!pip install beautifulsoup4 python-dateutil"
      ],
      "metadata": {
        "id": "xdncfhiTLno5",
        "collapsed": true,
        "outputId": "bc5eb01b-6c03-4a71-9f14-faa236b3d1d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil) (1.16.0)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to execute each time (imports used everywhere)\n",
        "!pip install python-dotenv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import json\n",
        "import time\n",
        "import dateutil\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "nJCM0UflGIE6",
        "outputId": "206f0399-0143-4e27-c04a-cfe53d1d28ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### New York Times Articles\n",
        "New York Times has its own API so we will use it. More specifically we will use the article search API."
      ],
      "metadata": {
        "id": "pv5vhOJx7kUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Execute this code before to create the .env file for the NYT API Key\n",
        "\n",
        "**If you want to retrieve New York Times articles**\n"
      ],
      "metadata": {
        "id": "wNUKNg-kNCko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('.env', 'w') as f:\n",
        "    f.write(\"API_KEY=W4DCpxZiGY6T8r2AP97p5gMQht3lF6gt\\n\")"
      ],
      "metadata": {
        "id": "jRc-jU3XF6b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Code for New York Times (This takes around 10 mins)"
      ],
      "metadata": {
        "id": "nNHiqcbWN2A1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib3\n",
        "from dotenv import load_dotenv\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "#load api key\n",
        "load_dotenv()\n",
        "\n",
        "####################\n",
        "####PARAMETERS######\n",
        "####################\n",
        "API_KEY = os.getenv('API_KEY')  # my API key retrieved from NYT API service\n",
        "NYT_base_url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json'# base URL for the NYT API\n",
        "\n",
        "# function to send a request to the NYT API\n",
        "def send_request(page):\n",
        "    params = {\n",
        "        'q': 'technology',\n",
        "        'api-key': API_KEY,\n",
        "        'page': page\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # make a GET request to the NYT API\n",
        "        response = requests.get(NYT_base_url, params=params, verify=False)\n",
        "        response.raise_for_status()  # raise an error for HTTP issues\n",
        "        time.sleep(12)  # delay to prevent rate-limiting (They allow 10 requests per minute so to make sure to respect that)\n",
        "        return response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for page {page}: {e}\")\n",
        "        return None\n",
        "\n",
        "# function to check if the article is well structured\n",
        "def is_valid(article):\n",
        "    # check if the article has a headline and main text\n",
        "    return 'headline' in article and 'main' in article['headline']\n",
        "\n",
        "# function to parse the response data into a DataFrame\n",
        "def parse_response(response):\n",
        "    # initialize data structure fields\n",
        "    data = {\n",
        "        'URL': [],\n",
        "        'Title': [],\n",
        "        'Author': [],\n",
        "        'Publication Date': [],\n",
        "        'Content': [], # content will be empty since the NYT doesn't give access to the full text\n",
        "        'Keywords': []\n",
        "    }\n",
        "\n",
        "    # extract information from each article\n",
        "    articles = response['response']['docs']\n",
        "    for article in articles:\n",
        "        if is_valid(article):\n",
        "            # append relevant data to the dictionary\n",
        "            data['URL'].append(article['web_url'])\n",
        "            data['Title'].append(article['headline']['main'])\n",
        "            data['Author'].append(article.get('byline', {}).get('original', \"No Author Found\"))\n",
        "            data['Publication Date'].append(dateutil.parser.parse(article['pub_date']).date())\n",
        "            data['Content'].append(\"\")  #empty since there is no text\n",
        "            keywords = [keyword['value'] for keyword in article['keywords'] if keyword['name'] == 'subject']\n",
        "            data['Keywords'].append(\", \".join(keywords) if keywords else \"No Keywords Found\")# store keywords as a comma-separated string\n",
        "\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# function to collect articles from multiple pages and save them as a single CSV\n",
        "def get_NYT_multiple_pages(start_page,nb_pages):\n",
        "    all_articles = pd.DataFrame()\n",
        "\n",
        "    #loop through multiple pages to fetch articles\n",
        "    for page_num in range(start_page, nb_pages):\n",
        "        print(f\"Fetching page {page_num + 1}...\")\n",
        "        response = send_request(page_num)\n",
        "        if response is not None:\n",
        "            page_data = parse_response(response)\n",
        "            all_articles = pd.concat([all_articles, page_data], ignore_index=True)\n",
        "        else:\n",
        "            print(f\"Skipping page {page_num + 1} due to errors.\")\n",
        "\n",
        "    #check and remove if any duplicate URLs\n",
        "    if 'URL' in all_articles.columns:\n",
        "        initial_count = len(all_articles)\n",
        "        all_articles = all_articles.drop_duplicates(subset='URL').reset_index(drop=True)\n",
        "        final_count = len(all_articles)\n",
        "        print(f\"Removed {initial_count - final_count} duplicate articles based on URL.\")\n",
        "    else:\n",
        "        print(\"No 'URL' column found. Skipping duplicate check.\")\n",
        "\n",
        "    #save the final DataFrame as a single CSV file\n",
        "    csv_path = \"/content/nyt_tech_articles2.csv\" #change the name for the different versions of the file\n",
        "    all_articles.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"Data saved to {csv_path}\")\n",
        "\n",
        "\n",
        "# specify the number of pages to scrape\n",
        "#get_NYT_multiple_pages(0,50) this was for the first 500 requests\n",
        "get_NYT_multiple_pages(50,100) # this is for the second 500 requests\n"
      ],
      "metadata": {
        "id": "nitzXANDyZPg",
        "collapsed": true,
        "outputId": "d8b29724-37e6-42a9-e72f-65797fa618d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching page 51...\n",
            "Fetching page 52...\n",
            "Fetching page 53...\n",
            "Fetching page 54...\n",
            "Fetching page 55...\n",
            "Fetching page 56...\n",
            "Fetching page 57...\n",
            "Fetching page 58...\n",
            "Fetching page 59...\n",
            "Fetching page 60...\n",
            "Fetching page 61...\n",
            "Fetching page 62...\n",
            "Fetching page 63...\n",
            "Fetching page 64...\n",
            "Fetching page 65...\n",
            "Fetching page 66...\n",
            "Fetching page 67...\n",
            "Fetching page 68...\n",
            "Fetching page 69...\n",
            "Fetching page 70...\n",
            "Fetching page 71...\n",
            "Fetching page 72...\n",
            "Fetching page 73...\n",
            "Fetching page 74...\n",
            "Fetching page 75...\n",
            "Fetching page 76...\n",
            "Fetching page 77...\n",
            "Fetching page 78...\n",
            "Fetching page 79...\n",
            "Fetching page 80...\n",
            "Fetching page 81...\n",
            "Fetching page 82...\n",
            "Fetching page 83...\n",
            "Fetching page 84...\n",
            "Fetching page 85...\n",
            "Fetching page 86...\n",
            "Fetching page 87...\n",
            "Fetching page 88...\n",
            "Fetching page 89...\n",
            "Fetching page 90...\n",
            "Fetching page 91...\n",
            "Fetching page 92...\n",
            "Fetching page 93...\n",
            "Fetching page 94...\n",
            "Fetching page 95...\n",
            "Fetching page 96...\n",
            "Fetching page 97...\n",
            "Fetching page 98...\n",
            "Fetching page 99...\n",
            "Fetching page 100...\n",
            "Removed 4 duplicate articles based on URL.\n",
            "Data saved to /content/nyt_tech_articles.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine two days worth of scraping (NYT API only allows 500 requests per day)"
      ],
      "metadata": {
        "id": "lSubVWNgM-PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#list of specific CSV files to combine (so we exclude NYT articles)\n",
        "csv_files = [\n",
        "    \"/content/nyt_tech_articles.csv\",\n",
        "    \"/content/nyt_tech_articles2.csv\"\n",
        "]\n",
        "\n",
        "output_file = \"combined_nyt_csvs.csv\"\n",
        "\n",
        "#list to hold DataFrames\n",
        "dataframes = []\n",
        "\n",
        "#loop through the list of the CSVs\n",
        "for file in csv_files:\n",
        "    print(f\"Reading {file}\")\n",
        "    #read each CSV file and append it to the list\n",
        "    df = pd.read_csv(file)\n",
        "    dataframes.append(df)\n",
        "\n",
        "#combine all DataFrames into one\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "#save the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv(output_file, index=False)\n",
        "print(f\"Combined CSV saved to {output_file}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0NdjuD1LM92K",
        "outputId": "7f040f9d-c8e2-4394-a323-ed500287f4a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading /content/nyt_tech_articles.csv\n",
            "Reading /content/nyt_tech_articles2.csv\n",
            "Combined CSV saved to combined_nyt_csvs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tech Crunch\n",
        "Tech Crunch is a news website dedicated to technological news. Therefore, we will crawl the latest news pages and scrape each article on the page. This snippet takes around 10 mins (for me)."
      ],
      "metadata": {
        "id": "gLpByc-tI8iA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Tech crunch also gives access to the sitemap so the crawling could have been done that way too.*"
      ],
      "metadata": {
        "id": "AAo0XfU_PODV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "####PARAMETERS######\n",
        "####################\n",
        "TC_base_url = \"https://techcrunch.com/latest/page/\" #base URL latest news\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"}#headers to pretend to be a browser\n",
        "\n",
        "def fetch_page(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    return response.text\n",
        "\n",
        "#function to parse article links from the page\n",
        "def parse_article_links(page_html):\n",
        "    soup = BeautifulSoup(page_html, 'html.parser')\n",
        "    article_links = []\n",
        "\n",
        "    #look for links with the correct class name\n",
        "    for article in soup.find_all('a', class_='loop-card__title-link'):\n",
        "        article_url = article.get('href')\n",
        "\n",
        "        #exclude links that contain '/podcast/'\n",
        "        if '/podcast/' not in article_url:\n",
        "            article_links.append(article_url)\n",
        "\n",
        "    return article_links\n",
        "\n",
        "#function to scrape the full text of an article and its components (title, author, date)\n",
        "def scrape_TC_article(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        title = soup.find('title')\n",
        "        title = title.text.strip() if title else \"No Title Found\"\n",
        "\n",
        "        author_meta = soup.find('meta', attrs={'name': 'author'})\n",
        "        author = author_meta['content'].strip() if author_meta else \"No Author Found\"\n",
        "\n",
        "        date = soup.find('meta', attrs={'property': 'article:published_time'})\n",
        "        date = dateutil.parser.parse(date['content']).date() if date else \"No Date Found\"\n",
        "\n",
        "        #try to find all <p> tags with the class 'wp-block-paragraph'\n",
        "        paragraphs = soup.find('div', attrs={'class': 'entry-content wp-block-post-content is-layout-constrained wp-block-post-content-is-layout-constrained'}).find_all('p')\n",
        "        content = \"\\n\".join(p.text.strip() for p in paragraphs if p.text)\n",
        "\n",
        "        return {'URL': url, 'Title': title, 'Author': author, 'Publication Date': date, 'Content': content}\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return {'URL': url, 'Title': None, 'Author': None, 'Publication Date': None, 'Content': None}\n",
        "\n",
        "#function to scrape multiple pages\n",
        "def scrape_TC_multiple_pages(start_page=1, end_page=5):\n",
        "    csv_filename = \"tc_tech_articles.csv\"\n",
        "    scraped_urls = set()  # Set to track scraped URLs and avoid duplicates\n",
        "\n",
        "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=['URL', 'Title', 'Author', 'Publication Date', 'Content'])\n",
        "        writer.writeheader()\n",
        "\n",
        "        for page_num in range(start_page, end_page + 1):\n",
        "            page_url = TC_base_url+f'{page_num}/'\n",
        "\n",
        "            #fetch the page\n",
        "            print(f\"Fetching page: {page_url}\")\n",
        "            page_html = fetch_page(page_url)\n",
        "\n",
        "            #extract article links from the page\n",
        "            article_urls = parse_article_links(page_html)\n",
        "\n",
        "            #scrape content from each article\n",
        "            for article_url in article_urls:\n",
        "                if article_url not in scraped_urls:  #check if URL is already scraped\n",
        "                    article_data = scrape_TC_article(article_url)\n",
        "                    if article_data['Content']:  #only save if content is non-empty\n",
        "                        writer.writerow(article_data)\n",
        "                        scraped_urls.add(article_url)  #add URL to the set\n",
        "                else:\n",
        "                  time.sleep(1)  # wait for 1 second between requests to avoid overloading server\n",
        "\n",
        "    print(f\"Scraped articles are saved to {csv_filename}\")\n",
        "\n",
        "#run the scraper\n",
        "scrape_TC_multiple_pages(start_page=1, end_page=51)\n"
      ],
      "metadata": {
        "id": "vHB6sCy9urg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_and_filter_articles(input_file, output_file):\n",
        "    with open(input_file, mode='r', encoding='utf-8') as infile, open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
        "        reader = csv.DictReader(infile)\n",
        "        writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        #pattern to match date in URL\n",
        "        date_pattern = re.compile(r'/\\d{4}/\\d{2}/\\d{2}/')\n",
        "\n",
        "        for row in reader:\n",
        "            if row['URL'] and date_pattern.search(row['URL']):\n",
        "                if row['Title']:\n",
        "                    row['Title'] = row['Title'].replace('| TechCrunch', '').strip()\n",
        "                    row['Title'] = row['Title'].replace('| Techcrunch', '').strip()\n",
        "                writer.writerow(row)\n",
        "\n",
        "    print(f\"Titles cleaned and filtered articles saved to {output_file}\")\n",
        "\n",
        "input_file = \"/content/tc_tech_articles.csv\"\n",
        "output_file = \"/content/tc_tech_articles_cleaned.csv\"\n",
        "clean_and_filter_articles(input_file, output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ0c-EDkbGTr",
        "outputId": "a58e2425-3b4a-43f0-c929-a6039a2381b9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Titles cleaned and filtered articles saved to /content/tc_tech_articles_cleaned.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gyumdNi_o1jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Venture Beats\n",
        "The sitemap for venturebeats archives the sites news everyday. So, we will crawl the sitemap will all the links to the archives, and then crawl each archive, and scrape all articles with the designed keyword (I choose all keywords for all the IT related categories on the main website). There are headers for each snippet of code, but this was added for Venture Beats in the first place, they are very strict with their Scraping/Crawling rules. This takes around 40 mins for 100 pages."
      ],
      "metadata": {
        "id": "RPyV7Tn5JYvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "####PARAMETERS######\n",
        "####################\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"}#headers to pretend to be a browser\n",
        "VB_sitemap_url = \"https://venturebeat.com/sitemap.xml\"\n",
        "\n",
        "#fetch main sitemap where all archives of the sitemap of everyday are stored\n",
        "def fetch_sitemap(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, \"xml\")\n",
        "    sitemap_links = [loc.text for loc in soup.find_all(\"loc\")]\n",
        "    return sitemap_links\n",
        "\n",
        "#filter tech related articles (so filter with keywords and take out additional content such as pictures, etc)\n",
        "def is_article_url(url):\n",
        "    exclude_keywords = [\"wp-content\", \".png\", \".jpg\", \".webp\"]\n",
        "    if any(keyword in url for keyword in exclude_keywords):\n",
        "        return False\n",
        "    include_keywords = [\"/ai/\", \"/data-infrastructure/\", \"/programming-development/\", \"/automation/\", \"/security/\"]\n",
        "    return any(keyword in url for keyword in include_keywords)\n",
        "\n",
        "#scrape each article\n",
        "def scrape_VB_article(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        title = soup.find('h1', class_='article-title') or soup.find('title')\n",
        "        title = title.text.strip() if title else \"No Title Found\"\n",
        "\n",
        "        author_meta = soup.find('meta', attrs={'name': 'author'})\n",
        "        author = author_meta['content'].strip() if author_meta else \"No Author Found\"\n",
        "\n",
        "        date = soup.find('time')\n",
        "        date = dateutil.parser.parse(date['datetime']).date() if date else \"No Date Found\"\n",
        "\n",
        "        article_content = soup.find(\"div\", class_=\"article-content\")\n",
        "        #remove unwanted elements with the specified class names\n",
        "        for unwanted in article_content.find_all([\"div\", \"form\"], class_=[\"post-boilerplate\", \"boilerplate-before\", \"boilerplate-after\"]):\n",
        "            unwanted.decompose()\n",
        "\n",
        "        if article_content:\n",
        "            #extract only the text from the relevant section, ensuring it is cleaned up\n",
        "            clean_text = article_content.get_text(separator=\"\\n\", strip=True)\n",
        "        else:\n",
        "            print(\"Article content not found!\")\n",
        "            clean_text = \"\"\n",
        "\n",
        "        return {'URL': url, 'Title': title, 'Author': author, 'Publication Date': date, 'Content': clean_text}\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return {'URL': url, 'Title': None, 'Author': None, 'Publication Date': None, 'Content': None}\n",
        "\n",
        "#fetch a certain number of days of archive on the main sitemap\n",
        "def fetch_daily_sitemaps(main_sitemap, num_days=100):\n",
        "    daily_sitemaps = []\n",
        "    for i in range(min(num_days, len(main_sitemap))):\n",
        "        daily_sitemaps.append(main_sitemap[i])\n",
        "    return daily_sitemaps\n",
        "\n",
        "def scrape_VB_multiple_pages(num_days=100):\n",
        "    main_sitemap = fetch_sitemap(VB_sitemap_url)\n",
        "    daily_sitemaps = fetch_daily_sitemaps(main_sitemap, num_days)\n",
        "    #collect articles url\n",
        "    all_urls = set()\n",
        "    for daily_sitemap in daily_sitemaps:\n",
        "        daily_urls = fetch_sitemap(daily_sitemap)\n",
        "        all_urls.update(daily_urls)\n",
        "        time.sleep(5)  #sleep to not overwhelm the server\n",
        "\n",
        "    #filter URLs with the filtering method\n",
        "    article_urls = [url for url in all_urls if is_article_url(url)]\n",
        "\n",
        "    #scrape the articles one by one\n",
        "    articles_data = []\n",
        "    for url in article_urls:\n",
        "        article_data = scrape_VB_article(url)\n",
        "        if article_data['Content']:  # Save only when they have content\n",
        "            articles_data.append(article_data)\n",
        "        time.sleep(5)\n",
        "\n",
        "    output_file = \"/content/vb_tech_articles.csv\"\n",
        "    df = pd.DataFrame(articles_data)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Scraping complete. Data saved to {output_file}.\")\n",
        "\n",
        "#run the scraper\n",
        "scrape_VB_multiple_pages(100)"
      ],
      "metadata": {
        "id": "xXxwBJu_4Mrc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Next Web"
      ],
      "metadata": {
        "id": "QIeVgxvFEK-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This scraper fetches and extracts article data from The Next Web's latest tech news section. It uses Requests to send HTTP requests and BeautifulSoup to parse HTML, targeting article links, then scraping content from each linked article. The results are saved in a CSV file, with small delays between requests to avoid overloading the server (no specifications in the robots.txt)."
      ],
      "metadata": {
        "id": "Z_gzp5VIQepB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "####################\n",
        "####PARAMETERS######\n",
        "####################\n",
        "crawl_TNW_base_url = \"https://thenextweb.com/latest\" #base url to crawl the website\n",
        "TNW_base_url = \"https://thenextweb.com\" #base url to create articles url to scrape\n",
        "\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}\n",
        "\n",
        "def fetch_page(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to fetch {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "#function to parse article links from the page\n",
        "def parse_article_links(page_html):\n",
        "    if not page_html:\n",
        "        return []\n",
        "    soup = BeautifulSoup(page_html, 'html.parser')\n",
        "    article_links = []\n",
        "\n",
        "    article_container = soup.find('div', id='articleList')\n",
        "    if not article_container:\n",
        "        print(\"Article list not found on the page.\")\n",
        "        return article_links\n",
        "\n",
        "    for article in article_container.find_all('a', class_='title_link', href=True):\n",
        "        article_url = article['href']\n",
        "        full_url = TNW_base_url + article_url if article_url.startswith('/') else article_url\n",
        "        article_links.append(full_url)\n",
        "\n",
        "    return article_links\n",
        "\n",
        "#scrape the data of individual article\n",
        "def scrape_TNW_article(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        title = soup.find('h1')\n",
        "        title = title.text.strip() if title else \"No Title Found\"\n",
        "\n",
        "        author_span = soup.find('span', class_='c-article__authorName latest')\n",
        "        author = author_span.text.strip() if author_span else \"No Author Found\"\n",
        "\n",
        "        date = soup.find('time')\n",
        "        date = dateutil.parser.parse(date['datetime']).date() if date and 'datetime' in date.attrs else \"No Date Found\"\n",
        "\n",
        "        paragraphs = soup.find('div', attrs={'id': 'article-main-content'})\n",
        "        if paragraphs:\n",
        "            paragraphs = paragraphs.find_all('p')\n",
        "            content = \"\\n\".join(p.text.strip() for p in paragraphs if p.text)\n",
        "        else:\n",
        "            content = \"No Content Found\"\n",
        "\n",
        "        time.sleep(0.3)  #slight delay between requests since no specifics in robots.txt\n",
        "        return {'URL': url, 'Title': title, 'Author': author, 'Publication Date': date, 'Content': content}\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return {'URL': url, 'Title': None, 'Author': None, 'Publication Date': None, 'Content': None}\n",
        "\n",
        "#function to scrape multiple pages\n",
        "def scrape_TNW_multiple_pages(start_page=1, end_page=5):\n",
        "    csv_filename = \"tnw_tech_articles.csv\"\n",
        "    scraped_urls = set()\n",
        "\n",
        "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=['URL', 'Title', 'Author', 'Publication Date', 'Content'])\n",
        "        writer.writeheader()\n",
        "\n",
        "        for page_num in range(start_page, end_page + 1):\n",
        "            page_url = f\"{crawl_TNW_base_url}/page/{page_num}/\"\n",
        "            print(f\"Fetching page: {page_url}\")\n",
        "            page_html = fetch_page(page_url)\n",
        "\n",
        "            article_urls = parse_article_links(page_html)\n",
        "            for article_url in article_urls:\n",
        "                if article_url not in scraped_urls:\n",
        "                    article_data = scrape_TNW_article(article_url)\n",
        "                    if article_data['Content']:\n",
        "                        writer.writerow(article_data)\n",
        "                        scraped_urls.add(article_url)\n",
        "\n",
        "    print(f\"Scraped articles are saved to {csv_filename}\")\n",
        "\n",
        "# Run the scraper\n",
        "scrape_TNW_multiple_pages(start_page=1, end_page=300)"
      ],
      "metadata": {
        "id": "iJgHoDiSEWSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each dataframe has been created according to the source code of each website, and has been checked for duplicates. The delay is also adapted to the robots.txt of the website if indicated."
      ],
      "metadata": {
        "id": "pbcCaWy6Ohyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Combine the CSVs (except for NYT) together"
      ],
      "metadata": {
        "id": "fUNxQA-nzMpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this code to work, you need to have all the CSVs in the content directory (I would recommend putting them directly and not executing the scraping code since it would take multiple hours)"
      ],
      "metadata": {
        "id": "ZGChoxWzGk6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#list of specific CSV files to combine (so we exclude NYT articles)\n",
        "csv_files = [\n",
        "    \"/content/tnw_tech_articles.csv\",\n",
        "    \"/content/tc_tech_articles_cleaned.csv\",\n",
        "    \"/content/vb_tech_articles.csv\"\n",
        "]\n",
        "\n",
        "output_file = \"combined_csvs.csv\"\n",
        "\n",
        "#list to hold DataFrames\n",
        "dataframes = []\n",
        "\n",
        "#loop through the list of the CSVs\n",
        "for file in csv_files:\n",
        "    print(f\"Reading {file}\")\n",
        "    #read each CSV file and append it to the list\n",
        "    df = pd.read_csv(file)\n",
        "    dataframes.append(df)\n",
        "\n",
        "#combine all DataFrames into one\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "#save the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv(output_file, index=False)\n",
        "print(f\"Combined CSV saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "O8nUyXiIzMF-",
        "outputId": "581c733d-24b8-486a-c1ff-62285d0a66f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading /content/tnw_tech_articles.csv\n",
            "Reading /content/tc_tech_articles_cleaned.csv\n",
            "Reading /content/vb_tech_articles.csv\n",
            "Combined CSV saved to combined_csvs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Data Preprocessing"
      ],
      "metadata": {
        "id": "VFatrMm3Kkgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_content_column(input_file, output_file, word_to_remove):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    word_pattern = re.compile(re.escape(word_to_remove), re.IGNORECASE)\n",
        "\n",
        "    with open(input_file, mode='r', encoding='utf-8') as infile, open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
        "        reader = csv.DictReader(infile)\n",
        "        writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        for row in reader:\n",
        "            if 'Content' in row and row['Content']:\n",
        "                #remove URLs\n",
        "                row['Content'] = url_pattern.sub('', row['Content'])\n",
        "                #remove the specified word (techcrunch initialy, but we can use later if need be)\n",
        "                row['Content'] = word_pattern.sub('', row['Content'])\n",
        "            writer.writerow(row)\n",
        "\n",
        "    print(f\"Links and the word '{word_to_remove}' removed from the 'Content' column. Cleaned data saved to {output_file}\")\n",
        "clean_content_column(\"combined_csvs.csv\", \"cleaned_combined_data.csv\", \"TechCrunch\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-rOtGgjfy26",
        "outputId": "d279bb5d-8fdc-490a-ecab-edfc4e07709f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Links and the word 'TechCrunch' removed from the 'Content' column. Cleaned data saved to cleaned_combined_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e2BXoPFEfz6z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}