{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AL6DdQVg5sDR"
   },
   "source": [
    "# IT News Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTOODAm5590e"
   },
   "source": [
    "The project aims to analyze IT news articles and recommend related articles to identify patterns, trends. This could help in understanding certain topics or discovering emerging themes in current events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAuG2qJ967pP"
   },
   "source": [
    "# Part 1: Scraping and Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzvOCE7E7EjH"
   },
   "source": [
    "We will use multiple websites to scrape the articles. Each website has a different snippet of code as the method to get the articles is adapted to each website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhyVuACsLgtE"
   },
   "source": [
    "####Code to execute before the others, all the other scraping snippets can be executed individually though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "xdncfhiTLno5",
    "outputId": "2149cbe4-2682-46a3-f6e5-38ccb4ebd416"
   },
   "outputs": [],
   "source": [
    "#for the first time launching google collab (packages to install)\n",
    "!pip install beautifulsoup4 python-dateutil datasketch scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJCM0UflGIE6"
   },
   "outputs": [],
   "source": [
    "#to execute each time (imports used everywhere)\n",
    "!pip install python-dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import dateutil\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pv5vhOJx7kUy"
   },
   "source": [
    "### New York Times Articles\n",
    "New York Times has its own API so we will use it. More specifically we will use the article search API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNUKNg-kNCko"
   },
   "source": [
    "####Execute this code before to create the .env file for the NYT API Key\n",
    "\n",
    "**If you want to retrieve New York Times articles**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRc-jU3XF6b1"
   },
   "outputs": [],
   "source": [
    "with open('.env', 'w') as f:\n",
    "    f.write(\"API_KEY=W4DCpxZiGY6T8r2AP97p5gMQht3lF6gt\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNHiqcbWN2A1"
   },
   "source": [
    "Main Code for New York Times (This takes around 10 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "nitzXANDyZPg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib3\n",
    "from dotenv import load_dotenv\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "#load api key\n",
    "load_dotenv()\n",
    "\n",
    "####################\n",
    "####PARAMETERS######\n",
    "####################\n",
    "API_KEY = os.getenv('API_KEY')  # my API key retrieved from NYT API service\n",
    "NYT_base_url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json'# base URL for the NYT API\n",
    "\n",
    "# function to send a request to the NYT API\n",
    "def send_request(page):\n",
    "    params = {\n",
    "        'q': 'technology',\n",
    "        'api-key': API_KEY,\n",
    "        'page': page\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # make a GET request to the NYT API\n",
    "        response = requests.get(NYT_base_url, params=params, verify=False)\n",
    "        response.raise_for_status()  # raise an error for HTTP issues\n",
    "        time.sleep(12)  # delay to prevent rate-limiting (They allow 10 requests per minute so to make sure to respect that)\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for page {page}: {e}\")\n",
    "        return None\n",
    "\n",
    "# function to check if the article is well structured\n",
    "def is_valid(article):\n",
    "    # check if the article has a headline and main text\n",
    "    return 'headline' in article and 'main' in article['headline']\n",
    "\n",
    "# function to parse the response data into a DataFrame\n",
    "def parse_response(response):\n",
    "    # initialize data structure fields\n",
    "    data = {\n",
    "        'URL': [],\n",
    "        'Title': [],\n",
    "        'Author': [],\n",
    "        'Publication Date': [],\n",
    "        'Content': [], # content will be empty since the NYT doesn't give access to the full text\n",
    "        'Keywords': []\n",
    "    }\n",
    "\n",
    "    # extract information from each article\n",
    "    articles = response['response']['docs']\n",
    "    for article in articles:\n",
    "        if is_valid(article):\n",
    "            # append relevant data to the dictionary\n",
    "            data['URL'].append(article['web_url'])\n",
    "            data['Title'].append(article['headline']['main'])\n",
    "            data['Author'].append(article.get('byline', {}).get('original', \"No Author Found\"))\n",
    "            data['Publication Date'].append(dateutil.parser.parse(article['pub_date']).date())\n",
    "            data['Content'].append(\"\")  #empty since there is no text\n",
    "            keywords = [keyword['value'] for keyword in article['keywords'] if keyword['name'] == 'subject']\n",
    "            data['Keywords'].append(\", \".join(keywords) if keywords else \"No Keywords Found\")# store keywords as a comma-separated string\n",
    "\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# function to collect articles from multiple pages and save them as a single CSV\n",
    "def get_NYT_multiple_pages(start_page,nb_pages):\n",
    "    all_articles = pd.DataFrame()\n",
    "\n",
    "    #loop through multiple pages to fetch articles\n",
    "    for page_num in range(start_page, nb_pages):\n",
    "        print(f\"Fetching page {page_num + 1}...\")\n",
    "        response = send_request(page_num)\n",
    "        if response is not None:\n",
    "            page_data = parse_response(response)\n",
    "            all_articles = pd.concat([all_articles, page_data], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Skipping page {page_num + 1} due to errors.\")\n",
    "\n",
    "    #check and remove if any duplicate URLs\n",
    "    if 'URL' in all_articles.columns:\n",
    "        initial_count = len(all_articles)\n",
    "        all_articles = all_articles.drop_duplicates(subset='URL').reset_index(drop=True)\n",
    "        final_count = len(all_articles)\n",
    "        print(f\"Removed {initial_count - final_count} duplicate articles based on URL.\")\n",
    "    else:\n",
    "        print(\"No 'URL' column found. Skipping duplicate check.\")\n",
    "\n",
    "    #save the final DataFrame as a single CSV file\n",
    "    csv_path = \"/content/nyt_tech_articles2.csv\" #change the name for the different versions of the file\n",
    "    all_articles.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Data saved to {csv_path}\")\n",
    "\n",
    "\n",
    "# specify the number of pages to scrape\n",
    "#get_NYT_multiple_pages(0,50) this was for the first 500 requests\n",
    "get_NYT_multiple_pages(50,100) # this is for the second 500 requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSubVWNgM-PS"
   },
   "source": [
    "Combine two days worth of scraping (NYT API only allows 500 requests per day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "0NdjuD1LM92K"
   },
   "outputs": [],
   "source": [
    "#list of specific CSV files to combine (so we exclude NYT articles)\n",
    "csv_files = [\n",
    "    \"/content/nyt_tech_articles.csv\",\n",
    "    \"/content/nyt_tech_articles2.csv\"\n",
    "]\n",
    "\n",
    "output_file = \"combined_nyt_csvs.csv\"\n",
    "\n",
    "#list to hold DataFrames\n",
    "dataframes = []\n",
    "\n",
    "#loop through the list of the CSVs\n",
    "for file in csv_files:\n",
    "    print(f\"Reading {file}\")\n",
    "    #read each CSV file and append it to the list\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes.append(df)\n",
    "\n",
    "#combine all DataFrames into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "#save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "print(f\"Combined CSV saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLpByc-tI8iA"
   },
   "source": [
    "### Tech Crunch\n",
    "Tech Crunch is a news website dedicated to technological news. Therefore, we will crawl the latest news pages and scrape each article on the page. This snippet takes around 10 mins (for me)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAo0XfU_PODV"
   },
   "source": [
    "*Tech crunch also gives access to the sitemap so the crawling could have been done that way too.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHB6sCy9urg4"
   },
   "outputs": [],
   "source": [
    "####################\n",
    "####PARAMETERS######\n",
    "####################\n",
    "TC_base_url = \"https://techcrunch.com/latest/page/\" #base URL latest news\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"}#headers to pretend to be a browser\n",
    "\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "#function to parse article links from the page\n",
    "def parse_article_links(page_html):\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    article_links = []\n",
    "\n",
    "    #look for links with the correct class name\n",
    "    for article in soup.find_all('a', class_='loop-card__title-link'):\n",
    "        article_url = article.get('href')\n",
    "\n",
    "        #exclude links that contain '/podcast/'\n",
    "        if '/podcast/' not in article_url:\n",
    "            article_links.append(article_url)\n",
    "\n",
    "    return article_links\n",
    "\n",
    "#function to scrape the full text of an article and its components (title, author, date)\n",
    "def scrape_TC_article(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        title = soup.find('title')\n",
    "        title = title.text.strip() if title else \"No Title Found\"\n",
    "\n",
    "        author_meta = soup.find('meta', attrs={'name': 'author'})\n",
    "        author = author_meta['content'].strip() if author_meta else \"No Author Found\"\n",
    "\n",
    "        date = soup.find('meta', attrs={'property': 'article:published_time'})\n",
    "        date = dateutil.parser.parse(date['content']).date() if date else \"No Date Found\"\n",
    "\n",
    "        #try to find all <p> tags with the class 'wp-block-paragraph'\n",
    "        paragraphs = soup.find('div', attrs={'class': 'entry-content wp-block-post-content is-layout-constrained wp-block-post-content-is-layout-constrained'}).find_all('p')\n",
    "        content = \"\\n\".join(p.text.strip() for p in paragraphs if p.text)\n",
    "\n",
    "        return {'URL': url, 'Title': title, 'Author': author, 'Publication Date': date, 'Content': content}\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return {'URL': url, 'Title': None, 'Author': None, 'Publication Date': None, 'Content': None}\n",
    "\n",
    "#function to scrape multiple pages\n",
    "def scrape_TC_multiple_pages(start_page=1, end_page=5):\n",
    "    csv_filename = \"tc_tech_articles.csv\"\n",
    "    scraped_urls = set()  # Set to track scraped URLs and avoid duplicates\n",
    "\n",
    "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['URL', 'Title', 'Author', 'Publication Date', 'Content'])\n",
    "        writer.writeheader()\n",
    "\n",
    "        for page_num in range(start_page, end_page + 1):\n",
    "            page_url = TC_base_url+f'{page_num}/'\n",
    "\n",
    "            #fetch the page\n",
    "            print(f\"Fetching page: {page_url}\")\n",
    "            page_html = fetch_page(page_url)\n",
    "\n",
    "            #extract article links from the page\n",
    "            article_urls = parse_article_links(page_html)\n",
    "\n",
    "            #scrape content from each article\n",
    "            for article_url in article_urls:\n",
    "                if article_url not in scraped_urls:  #check if URL is already scraped\n",
    "                    article_data = scrape_TC_article(article_url)\n",
    "                    if article_data['Content']:  #only save if content is non-empty\n",
    "                        writer.writerow(article_data)\n",
    "                        scraped_urls.add(article_url)  #add URL to the set\n",
    "                else:\n",
    "                  time.sleep(1)  # wait for 1 second between requests to avoid overloading server\n",
    "\n",
    "    print(f\"Scraped articles are saved to {csv_filename}\")\n",
    "\n",
    "#run the scraper\n",
    "scrape_TC_multiple_pages(start_page=1, end_page=51)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJ0c-EDkbGTr"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_and_filter_articles(input_file, output_file):\n",
    "    with open(input_file, mode='r', encoding='utf-8') as infile, open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        #pattern to match date in URL\n",
    "        date_pattern = re.compile(r'/\\d{4}/\\d{2}/\\d{2}/')\n",
    "\n",
    "        for row in reader:\n",
    "            if row['URL'] and date_pattern.search(row['URL']):\n",
    "                if row['Title']:\n",
    "                    row['Title'] = row['Title'].replace('| TechCrunch', '').strip()\n",
    "                    row['Title'] = row['Title'].replace('| Techcrunch', '').strip()\n",
    "                writer.writerow(row)\n",
    "\n",
    "    print(f\"Titles cleaned and filtered articles saved to {output_file}\")\n",
    "\n",
    "input_file = \"/content/tc_tech_articles.csv\"\n",
    "output_file = \"/content/tc_tech_articles_cleaned.csv\"\n",
    "clean_and_filter_articles(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPyV7Tn5JYvj"
   },
   "source": [
    "### Venture Beats\n",
    "The sitemap for venturebeats archives the sites news everyday. So, we will crawl the sitemap will all the links to the archives, and then crawl each archive, and scrape all articles with the designed keyword (I choose all keywords for all the IT related categories on the main website). There are headers for each snippet of code, but this was added for Venture Beats in the first place, they are very strict with their Scraping/Crawling rules. This takes around 40 mins for 100 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "xXxwBJu_4Mrc"
   },
   "outputs": [],
   "source": [
    "####################\n",
    "####PARAMETERS######\n",
    "####################\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"}#headers to pretend to be a browser\n",
    "VB_sitemap_url = \"https://venturebeat.com/sitemap.xml\"\n",
    "\n",
    "#fetch main sitemap where all archives of the sitemap of everyday are stored\n",
    "def fetch_sitemap(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"xml\")\n",
    "    sitemap_links = [loc.text for loc in soup.find_all(\"loc\")]\n",
    "    return sitemap_links\n",
    "\n",
    "#filter tech related articles (so filter with keywords and take out additional content such as pictures, etc)\n",
    "def is_article_url(url):\n",
    "    exclude_keywords = [\"wp-content\", \".png\", \".jpg\", \".webp\"]\n",
    "    if any(keyword in url for keyword in exclude_keywords):\n",
    "        return False\n",
    "    include_keywords = [\"/ai/\", \"/data-infrastructure/\", \"/programming-development/\", \"/automation/\", \"/security/\"]\n",
    "    return any(keyword in url for keyword in include_keywords)\n",
    "\n",
    "#scrape each article\n",
    "def scrape_VB_article(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        title = soup.find('h1', class_='article-title') or soup.find('title')\n",
    "        title = title.text.strip() if title else \"No Title Found\"\n",
    "\n",
    "        author_meta = soup.find('meta', attrs={'name': 'author'})\n",
    "        author = author_meta['content'].strip() if author_meta else \"No Author Found\"\n",
    "\n",
    "        date = soup.find('time')\n",
    "        date = dateutil.parser.parse(date['datetime']).date() if date else \"No Date Found\"\n",
    "\n",
    "        article_content = soup.find(\"div\", class_=\"article-content\")\n",
    "        #remove unwanted elements with the specified class names\n",
    "        for unwanted in article_content.find_all([\"div\", \"form\"], class_=[\"post-boilerplate\", \"boilerplate-before\", \"boilerplate-after\"]):\n",
    "            unwanted.decompose()\n",
    "\n",
    "        if article_content:\n",
    "            #extract only the text from the relevant section, ensuring it is cleaned up\n",
    "            clean_text = article_content.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            print(\"Article content not found!\")\n",
    "            clean_text = \"\"\n",
    "\n",
    "        return {'URL': url, 'Title': title, 'Author': author, 'Publication Date': date, 'Content': clean_text}\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return {'URL': url, 'Title': None, 'Author': None, 'Publication Date': None, 'Content': None}\n",
    "\n",
    "#fetch a certain number of days of archive on the main sitemap\n",
    "def fetch_daily_sitemaps(main_sitemap, num_days=100):\n",
    "    daily_sitemaps = []\n",
    "    for i in range(min(num_days, len(main_sitemap))):\n",
    "        daily_sitemaps.append(main_sitemap[i])\n",
    "    return daily_sitemaps\n",
    "\n",
    "def scrape_VB_multiple_pages(num_days=100):\n",
    "    main_sitemap = fetch_sitemap(VB_sitemap_url)\n",
    "    daily_sitemaps = fetch_daily_sitemaps(main_sitemap, num_days)\n",
    "    #collect articles url\n",
    "    all_urls = set()\n",
    "    for daily_sitemap in daily_sitemaps:\n",
    "        daily_urls = fetch_sitemap(daily_sitemap)\n",
    "        all_urls.update(daily_urls)\n",
    "        time.sleep(5)  #sleep to not overwhelm the server\n",
    "\n",
    "    #filter URLs with the filtering method\n",
    "    article_urls = [url for url in all_urls if is_article_url(url)]\n",
    "\n",
    "    #scrape the articles one by one\n",
    "    articles_data = []\n",
    "    for url in article_urls:\n",
    "        article_data = scrape_VB_article(url)\n",
    "        if article_data['Content']:  # Save only when they have content\n",
    "            articles_data.append(article_data)\n",
    "        time.sleep(5)\n",
    "\n",
    "    output_file = \"/content/vb_tech_articles.csv\"\n",
    "    df = pd.DataFrame(articles_data)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Scraping complete. Data saved to {output_file}.\")\n",
    "\n",
    "#run the scraper\n",
    "scrape_VB_multiple_pages(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIeVgxvFEK-r"
   },
   "source": [
    "#The Next Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_gzp5VIQepB"
   },
   "source": [
    "This scraper fetches and extracts article data from The Next Web's latest tech news section. It uses Requests to send HTTP requests and BeautifulSoup to parse HTML, targeting article links, then scraping content from each linked article. The results are saved in a CSV file, with small delays between requests to avoid overloading the server (no specifications in the robots.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJgHoDiSEWSB"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "####################\n",
    "####PARAMETERS######\n",
    "####################\n",
    "crawl_TNW_base_url = \"https://thenextweb.com/latest\" #base url to crawl the website\n",
    "TNW_base_url = \"https://thenextweb.com\" #base url to create articles url to scrape\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\"}\n",
    "\n",
    "def fetch_page(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "#function to parse article links from the page\n",
    "def parse_article_links(page_html):\n",
    "    if not page_html:\n",
    "        return []\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    article_links = []\n",
    "\n",
    "    article_container = soup.find('div', id='articleList')\n",
    "    if not article_container:\n",
    "        print(\"Article list not found on the page.\")\n",
    "        return article_links\n",
    "\n",
    "    for article in article_container.find_all('a', class_='title_link', href=True):\n",
    "        article_url = article['href']\n",
    "        full_url = TNW_base_url + article_url if article_url.startswith('/') else article_url\n",
    "        article_links.append(full_url)\n",
    "\n",
    "    return article_links\n",
    "\n",
    "#scrape the data of individual article\n",
    "def scrape_TNW_article(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        title = soup.find('h1')\n",
    "        title = title.text.strip() if title else \"No Title Found\"\n",
    "\n",
    "        author_span = soup.find('span', class_='c-article__authorName latest')\n",
    "        author = author_span.text.strip() if author_span else \"No Author Found\"\n",
    "\n",
    "        date = soup.find('time')\n",
    "        date = dateutil.parser.parse(date['datetime']).date() if date and 'datetime' in date.attrs else \"No Date Found\"\n",
    "\n",
    "        paragraphs = soup.find('div', attrs={'id': 'article-main-content'})\n",
    "        if paragraphs:\n",
    "            paragraphs = paragraphs.find_all('p')\n",
    "            content = \"\\n\".join(p.text.strip() for p in paragraphs if p.text)\n",
    "        else:\n",
    "            content = \"No Content Found\"\n",
    "\n",
    "        time.sleep(0.3)  #slight delay between requests since no specifics in robots.txt\n",
    "        return {'URL': url, 'Title': title, 'Author': author, 'Publication Date': date, 'Content': content}\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return {'URL': url, 'Title': None, 'Author': None, 'Publication Date': None, 'Content': None}\n",
    "\n",
    "#function to scrape multiple pages\n",
    "def scrape_TNW_multiple_pages(start_page=1, end_page=5):\n",
    "    csv_filename = \"tnw_tech_articles.csv\"\n",
    "    scraped_urls = set()\n",
    "\n",
    "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['URL', 'Title', 'Author', 'Publication Date', 'Content'])\n",
    "        writer.writeheader()\n",
    "\n",
    "        for page_num in range(start_page, end_page + 1):\n",
    "            page_url = f\"{crawl_TNW_base_url}/page/{page_num}/\"\n",
    "            print(f\"Fetching page: {page_url}\")\n",
    "            page_html = fetch_page(page_url)\n",
    "\n",
    "            article_urls = parse_article_links(page_html)\n",
    "            for article_url in article_urls:\n",
    "                if article_url not in scraped_urls:\n",
    "                    article_data = scrape_TNW_article(article_url)\n",
    "                    if article_data['Content']:\n",
    "                        writer.writerow(article_data)\n",
    "                        scraped_urls.add(article_url)\n",
    "\n",
    "    print(f\"Scraped articles are saved to {csv_filename}\")\n",
    "\n",
    "# Run the scraper\n",
    "scrape_TNW_multiple_pages(start_page=1, end_page=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbcCaWy6Ohyo"
   },
   "source": [
    "Each dataframe has been created according to the source code of each website, and has been checked for duplicates. The delay is also adapted to the robots.txt of the website if indicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUNxQA-nzMpf"
   },
   "source": [
    "#Combine the CSVs (except for NYT) together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGChoxWzGk6i"
   },
   "source": [
    "For this code to work, you need to have all the CSVs in the content directory (I would recommend putting them directly and not executing the scraping code since it would take multiple hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "O8nUyXiIzMF-"
   },
   "outputs": [],
   "source": [
    "#list of specific CSV files to combine (so we exclude NYT articles)\n",
    "csv_files = [\n",
    "    \"/content/tnw_tech_articles.csv\",\n",
    "    \"/content/tc_tech_articles_cleaned.csv\",\n",
    "    \"/content/vb_tech_articles.csv\"\n",
    "]\n",
    "\n",
    "output_file = \"combined_csvs.csv\"\n",
    "\n",
    "#list to hold DataFrames\n",
    "dataframes = []\n",
    "\n",
    "#loop through the list of the CSVs\n",
    "for file in csv_files:\n",
    "    print(f\"Reading {file}\")\n",
    "    #read each CSV file and append it to the list\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes.append(df)\n",
    "\n",
    "#combine all DataFrames into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "#save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "print(f\"Combined CSV saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFatrMm3Kkgr"
   },
   "source": [
    "# Part 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "re-P28I58jo7"
   },
   "source": [
    "This part serves in removing Unnecessary elements, like URLs within the content column, using regular expressions, string filtering and replacing the found elements with an empty string. TechCrunch’s name was used excessively (in over half of their articles), so it was filtered out to prevent skewed clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-rOtGgjfy26"
   },
   "outputs": [],
   "source": [
    "def clean_content_column(input_file, output_file, word_to_remove):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    word_pattern = re.compile(re.escape(word_to_remove), re.IGNORECASE)\n",
    "\n",
    "    with open(input_file, mode='r', encoding='utf-8') as infile, open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            if 'Content' in row and row['Content']:\n",
    "                #remove URLs\n",
    "                row['Content'] = url_pattern.sub('', row['Content'])\n",
    "                #remove the specified word (techcrunch initialy, but we can use later if need be)\n",
    "                row['Content'] = word_pattern.sub('', row['Content'])\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Links and the word '{word_to_remove}' removed from the 'Content' column. Cleaned data saved to {output_file}\")\n",
    "clean_content_column(\"combined_csvs.csv\", \"cleaned_combined_data.csv\", \"TechCrunch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDS1qD5W5ZK-"
   },
   "source": [
    "# Part 3: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8Q2AvobZ0Ck"
   },
   "source": [
    "### *Either execute Parts 1 and 2 beforehand, or use the cleaned_combined_data.csv (recommended)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wDgClyu823e"
   },
   "source": [
    "The goal of this feature is to categorize news articles into more detailed categories by data mining Technology news articles. For example, the existing technology or science tabs are divided into categories such as AI, quantum computing, and security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zerRpBCk1pSP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# Load the dataset\n",
    "news_data = pd.read_csv(\"cleaned_combined_data.csv\")\n",
    "\n",
    "# Ensure the publication date is in datetime format\n",
    "news_data['PublicationDate'] = pd.to_datetime(news_data['Publication Date'])\n",
    "\n",
    "'''\n",
    "# check news_data dataframe\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(news_data.head(10))\n",
    "print(news_data.columns)\n",
    "'''\n",
    "# DataPreparation\n",
    "## We don't need URL, Author, Publication Date Column.\n",
    "## Create the textdata field which is combination of Title and Content field\n",
    "news_data['textdata'] = news_data['Title'] + \" \" + news_data['Content']\n",
    "\n",
    "## stop_words option functions as delete 'the' or 'is' etc\n",
    "tfidf_data = TfidfVectorizer(stop_words='english')\n",
    "# TF-IDF process\n",
    "tfidf_matrix = tfidf_data.fit_transform(news_data['textdata'])\n",
    "\n",
    "# Get feature names and calculate TF-IDF sum for each term\n",
    "feature_names = tfidf_data.get_feature_names_out()\n",
    "tfidf_sum = tfidf_matrix.sum(axis=0).A1\n",
    "keywords_scores = zip(feature_names, tfidf_sum)\n",
    "\n",
    "# Sort the keywords by score and select the top N keywords\n",
    "sorted_keywords_scores = sorted(keywords_scores, key=lambda x: x[1], reverse=True)\n",
    "top_n_keywords = sorted_keywords_scores[:20]\n",
    "\n",
    "# Convert publication date to datetime format and extract year and month for grouping\n",
    "news_data['YearMonth'] = news_data['PublicationDate'].dt.to_period('M')\n",
    "\n",
    "# Use CountVectorizer for frequency analysis of the top keywords\n",
    "count_vectorizer = CountVectorizer(vocabulary=[keyword[0] for keyword in top_n_keywords])\n",
    "count_matrix = count_vectorizer.fit_transform(news_data['textdata'])\n",
    "\n",
    "# Convert to DataFrame and add time information\n",
    "count_df = pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "count_df['YearMonth'] = news_data['YearMonth']\n",
    "\n",
    "# Filter for the current year\n",
    "current_year = pd.Timestamp.now().year\n",
    "count_df = count_df[count_df['YearMonth'].dt.year == current_year]\n",
    "\n",
    "# Aggregate keyword counts by time period\n",
    "trend_df = count_df.groupby('YearMonth').sum()\n",
    "\n",
    "# Create line plots for each keyword (this is for the trends analysis)\n",
    "fig = go.Figure()\n",
    "for keyword in trend_df.columns:\n",
    "  fig.add_trace(go.Scatter(x=trend_df.index.astype(str), y=trend_df[keyword], mode='lines', name=keyword))\n",
    "fig.update_layout(title='Keyword Trends Over Time', xaxis_title='Time', yaxis_title='Frequency')\n",
    "pio.write_image(fig, \"Trends.png\")\n",
    "fig.show(renderer=\"png\")\n",
    "\n",
    "# Apply PCA\n",
    "## To apply PCA, make it array\n",
    "pca = PCA(n_components=10)\n",
    "pca_data = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# Apply Hierarchical Clustering\n",
    "## To make dendrogram, make link matrix\n",
    "pca2linkage = linkage(pca_data, method='ward')\n",
    "\n",
    "# Visualize dendrogram\n",
    "plt.figure(figsize=(10, 8))\n",
    "dendrogram(pca2linkage)\n",
    "plt.title(\"Dendrogram\")\n",
    "plt.xlabel(\"Data Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n",
    "\n",
    "## Select the number of cluster & get the centroid value\n",
    "hierarchical_clustering = AgglomerativeClustering(n_clusters=5, linkage='ward')\n",
    "hierarchical_data = hierarchical_clustering.fit_predict(pca_data)\n",
    "\n",
    "## Count hierarchical_data's cluster and make it to list (if there are 5 clusters => [0,1,2,3,4])\n",
    "unique_data = np.unique(hierarchical_data)\n",
    "\n",
    "# Calculate each of the clusters centroid\n",
    "centers_list = []\n",
    "for label in unique_data:\n",
    "    cluster_data = pca_data[hierarchical_data == label]\n",
    "    cluster_center = cluster_data.mean(axis=0)\n",
    "    centers_list.append(cluster_center)\n",
    "initial_centroid = np.array(centers_list)\n",
    "\n",
    "# Apply K-means\n",
    "kmeans = KMeans(n_clusters=len(unique_data), init=initial_centroid)\n",
    "kmeans_data = kmeans.fit_predict(pca_data)\n",
    "\n",
    "# Visualize\n",
    "plt.scatter(pca_data[:, 0], pca_data[:, 1], c=kmeans_data, s=10)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color='red', marker='x', s=100)\n",
    "plt.title(\"K-means Results\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.show()\n",
    "\n",
    "# Print Cluster names\n",
    "cluster_names = []\n",
    "\n",
    "for i in range(len(np.unique(kmeans_data))):\n",
    "    cluster_index = np.where(kmeans_data == i)[0]\n",
    "    cluster_data = tfidf_matrix[cluster_index]\n",
    "    # Calculate TF-IDF average (make it A1 numpy array)\n",
    "    cluster_mean_tfidf = cluster_data.mean(axis=0).A1\n",
    "    # Top-5 words\n",
    "    top_keywords_array = np.argsort(cluster_mean_tfidf)[-5:]\n",
    "    names = tfidf_data.get_feature_names_out()\n",
    "    top_keywords = []\n",
    "    for j in top_keywords_array:\n",
    "        top_keywords.append(names[j])\n",
    "    cluster_keywords = \" & \".join(top_keywords)\n",
    "    cluster_names.append(cluster_keywords)\n",
    "print(\"Cluster KeyWords:\", cluster_names)\n",
    "\n",
    "# Output - Print Content(URLs) of the Clusters\n",
    "cluster_names_column = []\n",
    "for i in kmeans_data:\n",
    "    cluster_names_column.append(cluster_names[i])\n",
    "news_data['ClusterName'] = cluster_names_column\n",
    "for i, j in enumerate(cluster_names):\n",
    "    cluster_urls = news_data[news_data['ClusterName'] == j]['URL']\n",
    "    print(f\"\\nCluster '{j}':\")\n",
    "    print(cluster_urls.tolist())\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "news_data['Cluster'] = hierarchical_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_ETZqBUxSZT"
   },
   "source": [
    "# Part 4: LSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9C26RopZWKN"
   },
   "source": [
    "### *needs Part 3 executed beforehand*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IWdOzz09Hps"
   },
   "source": [
    "The purpose of this part is to provide efficient and accurate recommendations of similar articles based on their content using Locality-Sensitive Hashing (LSH) with MinHash, which allows us to approximate the similarity between articles in an efficient manner, without needing ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4zz6oJU4JCA"
   },
   "outputs": [],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "#define function to create MinHash\n",
    "def get_minhash(doc):\n",
    "    tokens = doc.split()\n",
    "    m = MinHash(num_perm=128)\n",
    "    for token in tokens:\n",
    "        m.update(token.encode('utf8'))\n",
    "    return m\n",
    "\n",
    "#create LSH for each cluster\n",
    "clusters = news_data['Cluster'].unique()\n",
    "lsh_dict = {cluster: MinHashLSH(threshold=0.1, num_perm=128) for cluster in clusters}\n",
    "\n",
    "#calculate MinHash for each document and insert into respective LSH\n",
    "minhashes = {cluster: [] for cluster in clusters}\n",
    "for idx, row in news_data.iterrows():\n",
    "    cluster = row['Cluster']\n",
    "    doc = row['textdata']\n",
    "    minhash = get_minhash(doc)\n",
    "    lsh_dict[cluster].insert(f\"doc_{idx}\", minhash)\n",
    "    minhashes[cluster].append((f\"doc_{idx}\", minhash))\n",
    "\n",
    "#function to find similar articles within the same cluster\n",
    "def find_similar_articles(article_id, cluster):\n",
    "    query_minhash = minhashes[cluster][article_id][1]\n",
    "    similar_docs = lsh_dict[cluster].query(query_minhash)\n",
    "    return similar_docs\n",
    "\n",
    "#example: find similar articles for the first article in its cluster\n",
    "article_id = 0\n",
    "cluster = news_data.iloc[article_id]['Cluster']\n",
    "similar_articles = find_similar_articles(article_id, cluster)\n",
    "print(f\"Similar articles to article {article_id} in cluster {cluster}: {similar_articles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tV49Lc-07S1x"
   },
   "source": [
    "# Part 5: App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1jSNibPZl0f"
   },
   "source": [
    "### *needs Part 3 and Part 4 to be executed beforehand*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIrxRv1t9W7-"
   },
   "source": [
    "This is our User Application. It uses both the data analysis and recommendations aspect of our project. The trends category showcases a graph of the evolving trends this past year. The recommendation category asks the user to input a link, and will find the article in the dataset, and recommend articles based on their similarity, which was evaluated beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "e2BXoPFEfz6z"
   },
   "outputs": [],
   "source": [
    "!pip install dash\n",
    "!pip install jupyter-dash\n",
    "!pip install dash-bootstrap-components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9qCFWgv7eeW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import dash_bootstrap_components as dbc\n",
    "import base64\n",
    "\n",
    "with open(\"/content/Trends.png\", \"rb\") as image_file:\n",
    "    encoded_string = base64.b64encode(image_file.read()).decode()\n",
    "\n",
    "#load the dataset\n",
    "articles_df = pd.read_csv(\"cleaned_combined_data.csv\")\n",
    "\n",
    "#data Preparation\n",
    "articles_df['textdata'] = articles_df['Title'] + \" \" + articles_df['Content']\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(articles_df['textdata'])\n",
    "\n",
    "#create a mapping from URLs to index\n",
    "url_to_index = {url: idx for idx, url in enumerate(articles_df['URL'])}\n",
    "\n",
    "def get_article_by_url(url, url_to_index):\n",
    "    if url in url_to_index:\n",
    "        return url_to_index[url]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def compute_similarity(tfidf_matrix, index):\n",
    "    query_vector = tfidf_matrix[index]\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    return similarities\n",
    "\n",
    "def recommend_articles(url, url_to_index, articles_df, tfidf_matrix, top_n=5):\n",
    "    index = get_article_by_url(url, url_to_index)\n",
    "    if index is not None:\n",
    "        similarities = compute_similarity(tfidf_matrix, index)\n",
    "        similar_indices = similarities.argsort()[::-1][1:top_n + 1]  # Exclude the input article itself\n",
    "        similar_urls = articles_df['URL'].iloc[similar_indices]\n",
    "        return similar_urls.tolist()\n",
    "    return []\n",
    "\n",
    "#dash App\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.LUX], suppress_callback_exceptions=True)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dcc.Location(id=\"url\", refresh=False),\n",
    "    html.Div(id=\"page-content\")\n",
    "])\n",
    "\n",
    "#page 1\n",
    "page_1_content = html.Div([\n",
    "    html.H3(\"Latest trends page\", style={'text-align': 'center', 'color': '#2980B9'}),\n",
    "    html.Img(src=f\"data:image/png;base64,{encoded_string}\", style={'display': 'block', 'margin-left': 'auto', 'margin-right': 'auto', 'width': '50%'}),\n",
    "    html.Div(\n",
    "        dcc.Link(\"Go Back to Homepage\", href=\"/\", style={'text-align': 'center', 'display': 'block', 'margin-top': '20px'}),\n",
    "        style={'text-align': 'center'}\n",
    "    ),\n",
    "])\n",
    "\n",
    "#page 2\n",
    "page_2_content = html.Div([\n",
    "    html.H3(\"Find a Matching Article\", style={'text-align': 'center', 'color': '#E74C3C'}),\n",
    "    html.P(\"Input a link below, and we'll find the best-matching article for you.\", style={'text-align': 'center', 'color': '#34495E'}),\n",
    "\n",
    "    #field for the link\n",
    "    html.Div([\n",
    "        dbc.Input(id=\"user-link-input\", type=\"url\", placeholder=\"Enter a link\", style={'width': '50%', 'margin': '0 auto'}),\n",
    "        dbc.Button(\"Submit\", id=\"submit-link-button\", color=\"primary\", style={'margin-top': '20px'}),\n",
    "    ], style={'text-align': 'center'}),\n",
    "\n",
    "    html.Div(id=\"match-result\", style={'margin-top': '30px', 'text-align': 'center', 'color': '#34495E'}),\n",
    "\n",
    "    html.Div(\n",
    "        dcc.Link(\"Go Back to Homepage\", href=\"/\", style={'text-align': 'center', 'display': 'block', 'margin-top': '20px'}),\n",
    "        style={'text-align': 'center'}\n",
    "    ),\n",
    "])\n",
    "\n",
    "#homepage\n",
    "homepage_content = html.Div([\n",
    "    html.H1(\"Tech News trend analysis and recommendations\", style={'text-align': 'center', 'color': '#2C3E50'}),\n",
    "    html.Div([\n",
    "        dbc.Button(\n",
    "            dcc.Link(\"Trends\", href=\"/page-1\", style={'text-decoration': 'none', 'color': 'white'}),\n",
    "            color=\"primary\", size=\"lg\", className=\"mr-2\"\n",
    "        ),\n",
    "        dbc.Button(\n",
    "            dcc.Link(\"Get a recommendation\", href=\"/page-2\", style={'text-decoration': 'none', 'color': 'black'}),\n",
    "            color=\"secondary\", size=\"lg\"\n",
    "        ),\n",
    "    ], style={'text-align': 'center', 'margin-top': '30px'}),\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"page-content\", \"children\"),\n",
    "    [Input(\"url\", \"pathname\")]\n",
    ")\n",
    "def display_page(pathname):\n",
    "    if pathname == \"/page-1\":\n",
    "        return page_1_content\n",
    "    elif pathname == \"/page-2\":\n",
    "        return page_2_content\n",
    "    return homepage_content\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"match-result\", \"children\"),\n",
    "    [Input(\"submit-link-button\", \"n_clicks\")],\n",
    "    [dash.dependencies.State(\"user-link-input\", \"value\")]\n",
    ")\n",
    "def find_article_match(n_clicks, user_link):\n",
    "    if n_clicks and user_link:\n",
    "        try:\n",
    "            recommended_urls = recommend_articles(user_link, url_to_index, articles_df, tfidf_matrix)\n",
    "            if recommended_urls:\n",
    "                matched_article = f\"Matched articles for the link: {user_link}\"\n",
    "                links = [html.Div(html.A(href=url, children=url, target=\"_blank\", style={'color': '#27AE60'}), style={'margin-bottom': '10px'}) for url in recommended_urls]\n",
    "                return html.Div([\n",
    "                    html.H4(\"Match Found!\", style={'color': '#27AE60'}),\n",
    "                    html.P(matched_article, style={'color': '#27AE60'}),\n",
    "                    html.Div(links, style={'margin-top': '20px'})\n",
    "                ])\n",
    "            else:\n",
    "                return html.Div([\n",
    "                    html.H4(\"No Matches Found\", style={'color': '#E74C3C'}),\n",
    "                    html.P(\"No matching articles were found for the provided link.\"),\n",
    "                ])\n",
    "        except Exception as e:\n",
    "            return html.Div([\n",
    "                html.H4(\"Error\", style={'color': '#E74C3C'}),\n",
    "                html.P(f\"An error occurred: {str(e)}\"),\n",
    "            ])\n",
    "    return \"\"\n",
    "\n",
    "#run the app locally\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9sx4PhSrY-1"
   },
   "source": [
    "# Part 6: Data properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyGLOyqzulhZ"
   },
   "source": [
    "### *Can be executed indepedently from the previous parts*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JX-yRTwx9k0q"
   },
   "source": [
    "This is the code to find the basic properties of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVNPtmvgpdQe"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "#load the dataset\n",
    "news_data = pd.read_csv(\"cleaned_combined_data.csv\")\n",
    "\n",
    "\n",
    "# Calculate mean and variance of the length of articles\n",
    "news_data['text_length'] = news_data['Content'].apply(len)\n",
    "mean_length = news_data['text_length'].mean()\n",
    "variance_length = news_data['text_length'].var()\n",
    "\n",
    "# Identify the range of interest\n",
    "lower_bound = news_data['text_length'].quantile(0.01)\n",
    "upper_bound = news_data['text_length'].quantile(0.99)\n",
    "\n",
    "# Test for normality\n",
    "stat, p = shapiro(news_data['text_length'])\n",
    "normality_test = 'normal' if p > 0.05 else 'not normal'\n",
    "\n",
    "print(f\"Mean length of articles: {mean_length}\")\n",
    "print(f\"Variance of article lengths: {variance_length}\")\n",
    "print(f\"Normality test result: The distribution is {normality_test} (p-value = {p})\")\n",
    "\n",
    "# Identify the range of interest\n",
    "lower_bound = news_data['text_length'].quantile(0.01)\n",
    "upper_bound = news_data['text_length'].quantile(0.99)\n",
    "\n",
    "# Histogram of article lengths with focus on skewed data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(news_data['text_length'], kde=True)\n",
    "plt.xlim(lower_bound, upper_bound)\n",
    "plt.title('Histogram of Article Lengths')\n",
    "plt.xlabel('Article Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
