{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IT News Project"
      ],
      "metadata": {
        "id": "AL6DdQVg5sDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project aims to analyze IT news articles and recommend related articles to identify patterns, trends. This could help in understanding certain topics or discovering emerging themes in current events."
      ],
      "metadata": {
        "id": "hTOODAm5590e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Scraping and Crawling"
      ],
      "metadata": {
        "id": "eAuG2qJ967pP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use multiple websites to scrape the articles. Each website has a different snippet of code as the method to get the articles is adapted to each website."
      ],
      "metadata": {
        "id": "CzvOCE7E7EjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Code to execute before the others, all the other scraping snippets can be executed individually though"
      ],
      "metadata": {
        "id": "rhyVuACsLgtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for the first time launching google collab (packages to install)\n",
        "!pip install python-dotenv beautifulsoup4 python-dateutil"
      ],
      "metadata": {
        "id": "xdncfhiTLno5",
        "collapsed": true,
        "outputId": "bc5eb01b-6c03-4a71-9f14-faa236b3d1d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil) (1.16.0)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to execute each time (imports used everywhere)\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import json\n",
        "import time\n",
        "import dateutil\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "nJCM0UflGIE6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### New York Times Articles\n",
        "New York Times has its own API so we will use it. More specifically we will use the article search API."
      ],
      "metadata": {
        "id": "pv5vhOJx7kUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Execute this code before to create the .env file for the NYT API Key\n",
        "\n",
        "**If you want to retrieve New York Times articles**\n"
      ],
      "metadata": {
        "id": "wNUKNg-kNCko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('.env', 'w') as f:\n",
        "    f.write(\"API_KEY=W4DCpxZiGY6T8r2AP97p5gMQht3lF6gt\\n\")"
      ],
      "metadata": {
        "id": "jRc-jU3XF6b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Code for New York Times (This takes around 10 mins)"
      ],
      "metadata": {
        "id": "nNHiqcbWN2A1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib3\n",
        "from dotenv import load_dotenv\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "#load api key\n",
        "load_dotenv()\n",
        "\n",
        "####################\n",
        "####PARAMETERS######\n",
        "####################\n",
        "API_KEY = os.getenv('API_KEY')  # my API key retrieved from NYT API service\n",
        "NYT_base_url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json'# base URL for the NYT API\n",
        "\n",
        "# function to send a request to the NYT API\n",
        "def send_request(page):\n",
        "    params = {\n",
        "        'q': 'technology',\n",
        "        'api-key': API_KEY,\n",
        "        'page': page\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # make a GET request to the NYT API\n",
        "        response = requests.get(NYT_base_url, params=params, verify=False)\n",
        "        response.raise_for_status()  # raise an error for HTTP issues\n",
        "        time.sleep(12)  # delay to prevent rate-limiting (They allow 10 requests per minute so to make sure to respect that)\n",
        "        return response.json()\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for page {page}: {e}\")\n",
        "        return None\n",
        "\n",
        "# function to check if the article is well structured\n",
        "def is_valid(article):\n",
        "    # check if the article has a headline and main text\n",
        "    return 'headline' in article and 'main' in article['headline']\n",
        "\n",
        "# function to parse the response data into a DataFrame\n",
        "def parse_response(response):\n",
        "    # initialize data structure fields\n",
        "    data = {\n",
        "        'URL': [],\n",
        "        'Title': [],\n",
        "        'Author': [],\n",
        "        'Publication Date': [],\n",
        "        'Content': [], # content will be empty since the NYT doesn't give access to the full text\n",
        "        'Keywords': []\n",
        "    }\n",
        "\n",
        "    # extract information from each article\n",
        "    articles = response['response']['docs']\n",
        "    for article in articles:\n",
        "        if is_valid(article):\n",
        "            # append relevant data to the dictionary\n",
        "            data['URL'].append(article['web_url'])\n",
        "            data['Title'].append(article['headline']['main'])\n",
        "            data['Author'].append(article.get('byline', {}).get('original', \"No Author Found\"))\n",
        "            data['Publication Date'].append(dateutil.parser.parse(article['pub_date']).date())\n",
        "            data['Content'].append(\"\")  #empty since there is no text\n",
        "            keywords = [keyword['value'] for keyword in article['keywords'] if keyword['name'] == 'subject']\n",
        "            data['Keywords'].append(\", \".join(keywords) if keywords else \"No Keywords Found\")# store keywords as a comma-separated string\n",
        "\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# function to collect articles from multiple pages and save them as a single CSV\n",
        "def get_NYT_multiple_pages(nb_pages):\n",
        "    all_articles = pd.DataFrame()\n",
        "\n",
        "    #loop through multiple pages to fetch articles\n",
        "    for page_num in range(nb_pages):\n",
        "        print(f\"Fetching page {page_num + 1}...\")\n",
        "        response = send_request(page_num)\n",
        "        if response is not None:\n",
        "            page_data = parse_response(response)\n",
        "            all_articles = pd.concat([all_articles, page_data], ignore_index=True)\n",
        "        else:\n",
        "            print(f\"Skipping page {page_num + 1} due to errors.\")\n",
        "\n",
        "    #check and remove if any duplicate URLs\n",
        "    if 'URL' in all_articles.columns:\n",
        "        initial_count = len(all_articles)\n",
        "        all_articles = all_articles.drop_duplicates(subset='URL').reset_index(drop=True)\n",
        "        final_count = len(all_articles)\n",
        "        print(f\"Removed {initial_count - final_count} duplicate articles based on URL.\")\n",
        "    else:\n",
        "        print(\"No 'URL' column found. Skipping duplicate check.\")\n",
        "\n",
        "    #save the final DataFrame as a single CSV file\n",
        "    csv_path = \"/content/nyt_tech_articles.csv\"\n",
        "    all_articles.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"Data saved to {csv_path}\")\n",
        "\n",
        "\n",
        "# specify the number of pages to scrape\n",
        "get_NYT_multiple_pages(50)\n"
      ],
      "metadata": {
        "id": "nitzXANDyZPg",
        "collapsed": true,
        "outputId": "1556be53-1772-45a9-bd29-ed367421c98e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "Fetching page 4...\n",
            "Fetching page 5...\n",
            "Fetching page 6...\n",
            "Fetching page 7...\n",
            "Fetching page 8...\n",
            "Fetching page 9...\n",
            "Fetching page 10...\n",
            "Fetching page 11...\n",
            "Fetching page 12...\n",
            "Fetching page 13...\n",
            "Fetching page 14...\n",
            "Fetching page 15...\n",
            "Fetching page 16...\n",
            "Fetching page 17...\n",
            "Fetching page 18...\n",
            "Fetching page 19...\n",
            "Fetching page 20...\n",
            "Fetching page 21...\n",
            "Fetching page 22...\n",
            "Fetching page 23...\n",
            "Fetching page 24...\n",
            "Fetching page 25...\n",
            "Fetching page 26...\n",
            "Fetching page 27...\n",
            "Fetching page 28...\n",
            "Fetching page 29...\n",
            "Fetching page 30...\n",
            "Fetching page 31...\n",
            "Fetching page 32...\n",
            "Fetching page 33...\n",
            "Fetching page 34...\n",
            "Fetching page 35...\n",
            "Fetching page 36...\n",
            "Fetching page 37...\n",
            "Fetching page 38...\n",
            "Fetching page 39...\n",
            "Fetching page 40...\n",
            "Fetching page 41...\n",
            "Fetching page 42...\n",
            "Fetching page 43...\n",
            "Fetching page 44...\n",
            "Fetching page 45...\n",
            "Fetching page 46...\n",
            "Fetching page 47...\n",
            "Fetching page 48...\n",
            "Fetching page 49...\n",
            "Fetching page 50...\n",
            "Removed 0 duplicate articles based on URL.\n",
            "Data saved to /content/nyt_tech_articles.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Tech Articles\n",
        "We will scrape the articles from the Tech page of CNN. The CNN sitemap doesn't keep any tech related articles, and scraping/crawling on the /search is forbidden (as written on the robots.txt). This takes around 15s (only one page to crawl)"
      ],
      "metadata": {
        "id": "KMpKyJKVIcc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "####################\n",
        "####PARAMETERS######\n",
        "####################\n",
        "CNN_base_url = \"https://edition.cnn.com/business/tech\" #Tech section base URL\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"}#headers to pretend to be a browser\n",
        "CNN_url = \"https://edition.cnn.com\" #URL to construct the url manually just in case\n",
        "\n",
        "#function to extract article URLs from CNN's Business Tech section\n",
        "def get_cnn_tech_article_urls(page_num=1):\n",
        "    # construct the URL for the page\n",
        "    url = f\"{CNN_base_url}?page={page_num}\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "    article_links = set()  #use a set to automatically handle duplicates\n",
        "\n",
        "    #check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        #find all article links\n",
        "        for link in soup.find_all(\"a\", href=True):  #finds all <a> tags with an href attribute\n",
        "            href = link[\"href\"]\n",
        "\n",
        "            #look for articles with \"/tech/\" in the URL (tech-related articles)\n",
        "            if \"/2024/\" in href and \"/tech/\" in href:\n",
        "                full_url = href if href.startswith(\"http\") else CNN_url + href\n",
        "                article_links.add(full_url)\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to retrieve page {page_num} at {url}\")\n",
        "\n",
        "    return article_links\n",
        "\n",
        "#function to extract article text from JSON-LD script tag\n",
        "def extract_text_from_json_ld(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        json_ld_tag = soup.find(\"script\", type=\"application/ld+json\")\n",
        "        if json_ld_tag:\n",
        "            json_data = json.loads(json_ld_tag.string)\n",
        "            if isinstance(json_data, list):\n",
        "                for item in json_data:\n",
        "                    if \"articleBody\" in item:\n",
        "                        return item[\"articleBody\"]\n",
        "            elif \"articleBody\" in json_data:\n",
        "                return json_data[\"articleBody\"]\n",
        "        print(f\"No article body found in JSON-LD for {url}\")\n",
        "        return \"\"\n",
        "    else:\n",
        "        print(f\"Failed to retrieve page at {url}\")\n",
        "        return \"\"\n",
        "\n",
        "#function to scrape the full text of an article\n",
        "def scrape_CNN_article(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        title = soup.find('title')\n",
        "        title = title.text.strip() if title else \"No Title Found\"\n",
        "\n",
        "        author_meta = soup.find('meta', attrs={'name': 'author'})\n",
        "        author = author_meta['content'].strip() if author_meta else \"No Author Found\"\n",
        "\n",
        "        date = soup.find('meta', attrs={'property': 'article:published_time'})\n",
        "        date = dateutil.parser.parse(date['content']).date() if date else \"No Date Found\"\n",
        "\n",
        "        paragraphs = soup.find('div', attrs={'class': 'article__content'}).find_all('p')\n",
        "        content = \"\\n\".join(p.text.strip() for p in paragraphs if p.text)\n",
        "\n",
        "        return {'URL': url, 'Title': title, 'Author': author, 'Publication Date': date, 'Content': content}\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return {'URL': url, 'Title': None, 'Author': None, 'Publication Date': None, 'Content': None}\n",
        "\n",
        "#function to save articles to a CSV file\n",
        "def save_to_csv(articles, filename=\"cnn_tech_articles.csv\"):\n",
        "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=[\"URL\",\"Title\", \"Author\", \"Publication Date\", \"Content\"])\n",
        "        writer.writeheader()\n",
        "        for article in articles:\n",
        "            writer.writerow(article)\n",
        "\n",
        "def scrape_CNN_multiple_pages(num_pages=1):\n",
        "    article_urls = set()\n",
        "    articles = []\n",
        "    for page_num in range(1, num_pages+1):\n",
        "      article_urls.update(get_cnn_tech_article_urls(page_num))\n",
        "    article_urls = list(article_urls)[:100]\n",
        "    for url in article_urls:\n",
        "      articles.append(scrape_CNN_article(url))\n",
        "      time.sleep(1)  #pause to avoid overwhelming the server\n",
        "    save_to_csv(articles)\n",
        "    print(\"Articles saved to cnn_tech_articles.csv\")\n",
        "\n",
        "#run the scraper\n",
        "#(I only fetch the first page because CNN doesn't store more than the current articles on the main tech page)\n",
        "scrape_CNN_multiple_pages(1)"
      ],
      "metadata": {
        "id": "VU9zCAiO4KZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tech Crunch\n",
        "Tech Crunch is a news website dedicated to technological news. Therefore, we will crawl the latest news pages and scrape each article on the page. This snippet takes around 10 mins (for me)."
      ],
      "metadata": {
        "id": "gLpByc-tI8iA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Tech crunch also gives access to the sitemap so the crawling could have been done that way too.*"
      ],
      "metadata": {
        "id": "AAo0XfU_PODV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "####PARAMETERS######\n",
        "####################\n",
        "TC_base_url = \"https://techcrunch.com/latest/page/\" #base URL latest news\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"}#headers to pretend to be a browser\n",
        "\n",
        "def fetch_page(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    return response.text\n",
        "\n",
        "#function to parse article links from the page\n",
        "def parse_article_links(page_html):\n",
        "    soup = BeautifulSoup(page_html, 'html.parser')\n",
        "    article_links = []\n",
        "\n",
        "    #look for links with the correct class name\n",
        "    for article in soup.find_all('a', class_='loop-card__title-link'):\n",
        "        article_url = article.get('href')\n",
        "\n",
        "        #exclude links that contain '/podcast/'\n",
        "        if '/podcast/' not in article_url:\n",
        "            article_links.append(article_url)\n",
        "\n",
        "    return article_links\n",
        "\n",
        "#function to scrape the full text of an article\n",
        "def scrape_TC_article(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        title = soup.find('title')\n",
        "        title = title.text.strip() if title else \"No Title Found\"\n",
        "\n",
        "        author_meta = soup.find('meta', attrs={'name': 'author'})\n",
        "        author = author_meta['content'].strip() if author_meta else \"No Author Found\"\n",
        "\n",
        "        date = soup.find('meta', attrs={'property': 'article:published_time'})\n",
        "        date = dateutil.parser.parse(date['content']).date() if date else \"No Date Found\"\n",
        "\n",
        "        #try to find all <p> tags with the class 'wp-block-paragraph'\n",
        "        paragraphs = soup.find('div', attrs={'class': 'entry-content wp-block-post-content is-layout-constrained wp-block-post-content-is-layout-constrained'}).find_all('p')\n",
        "        content = \"\\n\".join(p.text.strip() for p in paragraphs if p.text)\n",
        "\n",
        "        return {'URL': url, 'Title': title, 'Author': author, 'Publication Date': date, 'Content': content}\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return {'URL': url, 'Title': None, 'Author': None, 'Publication Date': None, 'Content': None}\n",
        "\n",
        "#function to scrape multiple pages\n",
        "def scrape_TC_multiple_pages(start_page=1, end_page=5):\n",
        "    csv_filename = \"tc_tech_articles.csv\"\n",
        "    scraped_urls = set()  # Set to track scraped URLs and avoid duplicates\n",
        "\n",
        "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=['URL', 'Title', 'Author', 'Publication Date', 'Content'])\n",
        "        writer.writeheader()\n",
        "\n",
        "        for page_num in range(start_page, end_page + 1):\n",
        "            page_url = TC_base_url+f'{page_num}/'\n",
        "\n",
        "            #fetch the page\n",
        "            print(f\"Fetching page: {page_url}\")\n",
        "            page_html = fetch_page(page_url)\n",
        "\n",
        "            #extract article links from the page\n",
        "            article_urls = parse_article_links(page_html)\n",
        "\n",
        "            #scrape content from each article\n",
        "            for article_url in article_urls:\n",
        "                if article_url not in scraped_urls:  #check if URL is already scraped\n",
        "                    article_data = scrape_TC_article(article_url)\n",
        "                    if article_data['Content']:  #only save if content is non-empty\n",
        "                        writer.writerow(article_data)\n",
        "                        scraped_urls.add(article_url)  #add URL to the set\n",
        "                else:\n",
        "                  time.sleep(1)  # wait for 1 second between requests to avoid overloading server\n",
        "\n",
        "    print(f\"Scraped articles are saved to {csv_filename}\")\n",
        "\n",
        "# Run the scraper\n",
        "scrape_TC_multiple_pages(start_page=1, end_page=51)\n"
      ],
      "metadata": {
        "id": "vHB6sCy9urg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Venture Beats\n",
        "The sitemap for venturebeats archives the sites news everyday. So, we will crawl the sitemap will all the links to the archives, and then crawl each archive, and scrape all articles with the designed keyword (I choose all keywords for all the IT related categories on the main website). There are headers for each snippet of code, but this was added for Venture Beats in the first place, they are very strict with their Scraping/Crawling rules. This takes around 40 mins for 100 pages."
      ],
      "metadata": {
        "id": "RPyV7Tn5JYvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "####PARAMETERS######\n",
        "####################\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"}#headers to pretend to be a browser\n",
        "VB_sitemap_url = \"https://venturebeat.com/sitemap.xml\"\n",
        "\n",
        "#fetch main sitemap where all archives of the sitemap of everyday are stored\n",
        "def fetch_sitemap(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, \"xml\")\n",
        "    sitemap_links = [loc.text for loc in soup.find_all(\"loc\")]\n",
        "    return sitemap_links\n",
        "\n",
        "#filter tech related articles (so filter with keywords and take out additional content such as pictures, etc)\n",
        "def is_article_url(url):\n",
        "    exclude_keywords = [\"wp-content\", \".png\", \".jpg\", \".webp\"]\n",
        "    if any(keyword in url for keyword in exclude_keywords):\n",
        "        return False\n",
        "    include_keywords = [\"/ai/\", \"/data-infrastructure/\", \"/programming-development/\", \"/automation/\", \"/security/\"]\n",
        "    return any(keyword in url for keyword in include_keywords)\n",
        "\n",
        "#scrape each article\n",
        "def scrape_VB_article(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        title = soup.find('h1', class_='article-title') or soup.find('title')\n",
        "        title = title.text.strip() if title else \"No Title Found\"\n",
        "\n",
        "        author_meta = soup.find('meta', attrs={'name': 'author'})\n",
        "        author = author_meta['content'].strip() if author_meta else \"No Author Found\"\n",
        "\n",
        "        date = soup.find('time')\n",
        "        date = dateutil.parser.parse(date['datetime']).date() if date else \"No Date Found\"\n",
        "\n",
        "        article_content = soup.find(\"div\", class_=\"article-content\")\n",
        "        #remove unwanted elements with the specified class names\n",
        "        for unwanted in article_content.find_all([\"div\", \"form\"], class_=[\"post-boilerplate\", \"boilerplate-before\", \"boilerplate-after\"]):\n",
        "            unwanted.decompose()\n",
        "\n",
        "        if article_content:\n",
        "            #extract only the text from the relevant section, ensuring it is cleaned up\n",
        "            clean_text = article_content.get_text(separator=\"\\n\", strip=True)\n",
        "        else:\n",
        "            print(\"Article content not found!\")\n",
        "            clean_text = \"\"\n",
        "\n",
        "        return {'URL': url, 'Title': title, 'Author': author, 'Publication Date': date, 'Content': clean_text}\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return {'URL': url, 'Title': None, 'Author': None, 'Publication Date': None, 'Content': None}\n",
        "\n",
        "#fetch a certain number of days of archive on the main sitemap\n",
        "def fetch_daily_sitemaps(main_sitemap, num_days=100):\n",
        "    daily_sitemaps = []\n",
        "    for i in range(min(num_days, len(main_sitemap))):\n",
        "        daily_sitemaps.append(main_sitemap[i])\n",
        "    return daily_sitemaps\n",
        "\n",
        "def scrape_VB_multiple_pages(num_days=100):\n",
        "    main_sitemap = fetch_sitemap(VB_sitemap_url)\n",
        "    daily_sitemaps = fetch_daily_sitemaps(main_sitemap, num_days)\n",
        "    #collect articles url\n",
        "    all_urls = set()\n",
        "    for daily_sitemap in daily_sitemaps:\n",
        "        daily_urls = fetch_sitemap(daily_sitemap)\n",
        "        all_urls.update(daily_urls)\n",
        "        time.sleep(5)  #sleep to not overwhelm the server\n",
        "\n",
        "    #filter URLs with the filtering method\n",
        "    article_urls = [url for url in all_urls if is_article_url(url)]\n",
        "\n",
        "    #scrape the articles one by one\n",
        "    articles_data = []\n",
        "    for url in article_urls:\n",
        "        article_data = scrape_VB_article(url)\n",
        "        if article_data['Content']:  # Save only when they have content\n",
        "            articles_data.append(article_data)\n",
        "        time.sleep(5)\n",
        "\n",
        "    output_file = \"/content/vb_tech_articles.csv\"\n",
        "    df = pd.DataFrame(articles_data)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Scraping complete. Data saved to {output_file}.\")\n",
        "\n",
        "#run the scraper\n",
        "scrape_VB_multiple_pages(100)"
      ],
      "metadata": {
        "id": "xXxwBJu_4Mrc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each dataframe has been created according to the source code of each website, and has been checked for duplicates. The delay is also adapted to the robots.txt of the website if indicated."
      ],
      "metadata": {
        "id": "pbcCaWy6Ohyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Combine the CSVs (except for NYT) together"
      ],
      "metadata": {
        "id": "fUNxQA-nzMpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#list of specific CSV files to combine (so we exclude NYT articles)\n",
        "csv_files = [\n",
        "    \"/content/cnn_tech_articles.csv\",\n",
        "    \"/content/tc_tech_articles.csv\",\n",
        "    \"/content/vb_tech_articles.csv\"\n",
        "]\n",
        "\n",
        "output_file = \"combined_csvs.csv\"\n",
        "\n",
        "#list to hold DataFrames\n",
        "dataframes = []\n",
        "\n",
        "#loop through the list of the CSVs\n",
        "for file in csv_files:\n",
        "    print(f\"Reading {file}\")\n",
        "    #read each CSV file and append it to the list\n",
        "    df = pd.read_csv(file)\n",
        "    dataframes.append(df)\n",
        "\n",
        "#combine all DataFrames into one\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "#save the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv(output_file, index=False)\n",
        "print(f\"Combined CSV saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "O8nUyXiIzMF-",
        "outputId": "2bfb195c-8676-4644-f0dd-2f4aab945e1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading /content/cnn_tech_articles.csv\n",
            "Reading /content/tc_tech_articles.csv\n",
            "Reading /content/vb_tech_articles.csv\n",
            "Combined CSV saved to combined_csvs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Data Preprocessing"
      ],
      "metadata": {
        "id": "VFatrMm3Kkgr"
      }
    }
  ]
}